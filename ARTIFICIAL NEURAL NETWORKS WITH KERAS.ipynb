{
    "cells": [
        {
            "metadata": {
                "collapsed": true
            },
            "cell_type": "markdown",
            "source": "# ARTIFICIAL NEURAL NETWORKS USING KERAS"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## PERCEPTRON"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import Perceptron",
            "execution_count": 1,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "iris = load_iris()\nX, y = iris['data'][:, 2:4], (iris['target'] == 0).astype(np.int)",
            "execution_count": 2,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "per_clf = Perceptron()\nper_clf.fit(X,y)\ny_pred = per_clf.predict([[2, 0.5]])",
            "execution_count": 3,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n  FutureWarning)\n",
                    "name": "stderr"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## THE MULTILAYER PERCEPTRON AND BACKPROPAGATION"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import tensorflow as tf\nfrom tensorflow import keras\ntf.__version__",
            "execution_count": 4,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 4,
                    "data": {
                        "text/plain": "'1.13.1'"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "keras.__version__",
            "execution_count": 5,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 5,
                    "data": {
                        "text/plain": "'2.2.4-tf'"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "fashion_mnist = keras.datasets.fashion_mnist\n(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()",
            "execution_count": 6,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "X_train_full.shape",
            "execution_count": 7,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 7,
                    "data": {
                        "text/plain": "(60000, 28, 28)"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "X_train_full.dtype",
            "execution_count": 8,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 8,
                    "data": {
                        "text/plain": "dtype('uint8')"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "X_valid, X_train = X_train_full[:5000]/255.0, X_train_full[5000:]/255.0\ny_valid, y_train = y_train_full[:5000], y_train_full[5000:]",
            "execution_count": 9,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\nclass_names[y_train[0]]",
            "execution_count": 10,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 10,
                    "data": {
                        "text/plain": "'Coat'"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Creating a model using a Sequential API"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "model = keras.models.Sequential()\nmodel.add(keras.layers.Flatten(input_shape=[28,28]))\nmodel.add(keras.layers.Dense(300, activation=\"relu\"))\nmodel.add(keras.layers.Dense(100, activation=\"relu\"))\nmodel.add(keras.layers.Dense(10, activation=\"softmax\"))",
            "execution_count": 11,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "WARNING:tensorflow:From /opt/conda/envs/Python36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "model = keras.models.Sequential([keras.layers.Flatten(input_shape=[28,28]),\n                                 keras.layers.Dense(300, activation=\"relu\"),\n                                 keras.layers.Dense(100, activation=\"relu\"),\n                                 keras.layers.Dense(10, activation=\"softmax\")\n                                ])",
            "execution_count": 12,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "model.summary()",
            "execution_count": 13,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nflatten_1 (Flatten)          (None, 784)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 300)               235500    \n_________________________________________________________________\ndense_4 (Dense)              (None, 100)               30100     \n_________________________________________________________________\ndense_5 (Dense)              (None, 10)                1010      \n=================================================================\nTotal params: 266,610\nTrainable params: 266,610\nNon-trainable params: 0\n_________________________________________________________________\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "model.layers",
            "execution_count": 14,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 14,
                    "data": {
                        "text/plain": "[<tensorflow.python.keras.layers.core.Flatten at 0x7f71ba56c9e8>,\n <tensorflow.python.keras.layers.core.Dense at 0x7f71ba56cba8>,\n <tensorflow.python.keras.layers.core.Dense at 0x7f71ba56ca20>,\n <tensorflow.python.keras.layers.core.Dense at 0x7f71a0206cc0>]"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "hidden1 = model.layers[1]",
            "execution_count": 15,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "hidden1.name",
            "execution_count": 16,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 16,
                    "data": {
                        "text/plain": "'dense_3'"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "model.get_layer('dense_3') is hidden1",
            "execution_count": 17,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 17,
                    "data": {
                        "text/plain": "True"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "weights, biases = hidden1.get_weights()",
            "execution_count": 18,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "weights",
            "execution_count": 19,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 19,
                    "data": {
                        "text/plain": "array([[-0.05694522, -0.03538627,  0.07162678, ...,  0.07394211,\n        -0.06995127, -0.04036611],\n       [-0.06955814,  0.00916329, -0.02573035, ..., -0.03607965,\n         0.02096347,  0.02685624],\n       [-0.01326674, -0.04599821,  0.02321346, ..., -0.04119897,\n        -0.01145911, -0.0261678 ],\n       ...,\n       [ 0.00374067,  0.05549364, -0.06336515, ..., -0.04533087,\n         0.04111351,  0.04629237],\n       [-0.03745648, -0.0672619 , -0.06905165, ..., -0.05914446,\n         0.01951272,  0.06572717],\n       [ 0.02092694,  0.0091701 ,  0.0127853 , ...,  0.00034419,\n         0.01823462,  0.00644349]], dtype=float32)"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "weights.shape",
            "execution_count": 20,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 20,
                    "data": {
                        "text/plain": "(784, 300)"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "biases",
            "execution_count": 21,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 21,
                    "data": {
                        "text/plain": "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "biases.shape",
            "execution_count": 22,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 22,
                    "data": {
                        "text/plain": "(300,)"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Compiling the model"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"sgd\", metrics = [\"accuracy\"])",
            "execution_count": 23,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Training and evaluating the model"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "history = model.fit(X_train, y_train, epochs = 30, validation_data=(X_valid, y_valid))",
            "execution_count": 24,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Train on 55000 samples, validate on 5000 samples\nEpoch 1/30\n55000/55000 [==============================] - 31s 557us/sample - loss: 0.6924 - acc: 0.7726 - val_loss: 0.5210 - val_acc: 0.8252\nEpoch 2/30\n55000/55000 [==============================] - 28s 515us/sample - loss: 0.4831 - acc: 0.8324 - val_loss: 0.4532 - val_acc: 0.8498\nEpoch 3/30\n55000/55000 [==============================] - 28s 508us/sample - loss: 0.4413 - acc: 0.8460 - val_loss: 0.4062 - val_acc: 0.8610\nEpoch 4/30\n55000/55000 [==============================] - 27s 486us/sample - loss: 0.4138 - acc: 0.8550 - val_loss: 0.3908 - val_acc: 0.8664\nEpoch 5/30\n55000/55000 [==============================] - 28s 510us/sample - loss: 0.3956 - acc: 0.8614 - val_loss: 0.3893 - val_acc: 0.8654\nEpoch 6/30\n55000/55000 [==============================] - 27s 496us/sample - loss: 0.3787 - acc: 0.8659 - val_loss: 0.3962 - val_acc: 0.8638\nEpoch 7/30\n55000/55000 [==============================] - 28s 506us/sample - loss: 0.3653 - acc: 0.8702 - val_loss: 0.3591 - val_acc: 0.8778\nEpoch 8/30\n55000/55000 [==============================] - 29s 522us/sample - loss: 0.3549 - acc: 0.8731 - val_loss: 0.3668 - val_acc: 0.8708\nEpoch 9/30\n55000/55000 [==============================] - 28s 508us/sample - loss: 0.3437 - acc: 0.8779 - val_loss: 0.3460 - val_acc: 0.8800\nEpoch 10/30\n55000/55000 [==============================] - 29s 518us/sample - loss: 0.3344 - acc: 0.8814 - val_loss: 0.3485 - val_acc: 0.8776\nEpoch 11/30\n55000/55000 [==============================] - 27s 499us/sample - loss: 0.3279 - acc: 0.8825 - val_loss: 0.3357 - val_acc: 0.8812\nEpoch 12/30\n55000/55000 [==============================] - 27s 494us/sample - loss: 0.3189 - acc: 0.8870 - val_loss: 0.3402 - val_acc: 0.8800\nEpoch 13/30\n55000/55000 [==============================] - 28s 503us/sample - loss: 0.3117 - acc: 0.8880 - val_loss: 0.3293 - val_acc: 0.8844\nEpoch 14/30\n55000/55000 [==============================] - 28s 505us/sample - loss: 0.3061 - acc: 0.8908 - val_loss: 0.3337 - val_acc: 0.8854\nEpoch 15/30\n55000/55000 [==============================] - 26s 475us/sample - loss: 0.2992 - acc: 0.8923 - val_loss: 0.3530 - val_acc: 0.8700\nEpoch 16/30\n55000/55000 [==============================] - 27s 486us/sample - loss: 0.2932 - acc: 0.8929 - val_loss: 0.3188 - val_acc: 0.8898\nEpoch 17/30\n55000/55000 [==============================] - 30s 537us/sample - loss: 0.2875 - acc: 0.8960 - val_loss: 0.3191 - val_acc: 0.8910\nEpoch 18/30\n55000/55000 [==============================] - 26s 475us/sample - loss: 0.2811 - acc: 0.8987 - val_loss: 0.3360 - val_acc: 0.8786\nEpoch 19/30\n55000/55000 [==============================] - 27s 497us/sample - loss: 0.2771 - acc: 0.8996 - val_loss: 0.3159 - val_acc: 0.8858\nEpoch 20/30\n55000/55000 [==============================] - 27s 482us/sample - loss: 0.2717 - acc: 0.9025 - val_loss: 0.3051 - val_acc: 0.8916\nEpoch 21/30\n55000/55000 [==============================] - 29s 525us/sample - loss: 0.2661 - acc: 0.9025 - val_loss: 0.3219 - val_acc: 0.8848\nEpoch 22/30\n55000/55000 [==============================] - 28s 513us/sample - loss: 0.2603 - acc: 0.9064 - val_loss: 0.3200 - val_acc: 0.8848\nEpoch 23/30\n55000/55000 [==============================] - 26s 478us/sample - loss: 0.2566 - acc: 0.9074 - val_loss: 0.3110 - val_acc: 0.8912\nEpoch 24/30\n55000/55000 [==============================] - 30s 544us/sample - loss: 0.2530 - acc: 0.9080 - val_loss: 0.3019 - val_acc: 0.8916\nEpoch 25/30\n55000/55000 [==============================] - 29s 528us/sample - loss: 0.2483 - acc: 0.9103 - val_loss: 0.3051 - val_acc: 0.8938\nEpoch 26/30\n55000/55000 [==============================] - 26s 477us/sample - loss: 0.2433 - acc: 0.9128 - val_loss: 0.3177 - val_acc: 0.8846\nEpoch 27/30\n55000/55000 [==============================] - 30s 546us/sample - loss: 0.2393 - acc: 0.9129 - val_loss: 0.3049 - val_acc: 0.8870\nEpoch 28/30\n55000/55000 [==============================] - 27s 491us/sample - loss: 0.2354 - acc: 0.9143 - val_loss: 0.2953 - val_acc: 0.8930\nEpoch 29/30\n55000/55000 [==============================] - 27s 489us/sample - loss: 0.2325 - acc: 0.9156 - val_loss: 0.3112 - val_acc: 0.8888\nEpoch 30/30\n55000/55000 [==============================] - 28s 508us/sample - loss: 0.2280 - acc: 0.9179 - val_loss: 0.2992 - val_acc: 0.8936\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import pandas as pd\nimport matplotlib.pyplot as plt",
            "execution_count": 25,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "pd.DataFrame(history.history).plot(figsize = (8,5))\nplt.grid(True)\nplt.gca().set_ylim(0,1)\nplt.show()",
            "execution_count": 26,
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "<Figure size 576x360 with 1 Axes>",
                        "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAEzCAYAAAALosttAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xd8XFed///XmT6jkUaj3lwlWS5xw3YcdychxUkIyVJCAikkkMc+IIGFhV3Y8KXujyzJF3aBhUBYshAgbMJ+KSFOJ5HjlmI77lWWi3rXqE2f8/vjjkaSLduSI3tG1uf5eNzHbWdmzlzLeuvce+65SmuNEEIIIVKHKdkVEEIIIcRQEs5CCCFEipFwFkIIIVKMhLMQQgiRYiSchRBCiBQj4SyEEEKkmHOGs1LqCaVUs1Jq7xn2K6XUj5RSVUqp3Uqp9419NYUQQoiJYyQt518B159l/zqgPD7dDzz23qslhBBCTFznDGet9RtA+1mKfBB4UhveBDKVUoVjVUEhhBBiohmLa87FQM2g9dr4NiGEEEKcB8sYvIcaZtuwY4Iqpe7HOPWNw+FYNHny5DH4+IkjFothMkkfvtGQYzZ6csxGT47Z6E3EY3b48OFWrXXuSMqORTjXApMGrZcA9cMV1Fo/DjwOUFFRoQ8dOjQGHz9xVFZWsnbt2mRXY1yRYzZ6csxGT47Z6E3EY6aUOjHSsmPxZ8uzwF3xXttXAD6tdcMYvK8QQggxIZ2z5ayU+j2wFshRStUC3wCsAFrrnwHPAzcAVUAf8MkLVVkhhBBiIjhnOGutbz/Hfg18dsxqJIQQQkxwE+tqvBBCCDEOSDgLIYQQKUbCWQghhEgxEs5CCCFEipFwFkIIIVKMhLMQQgiRYiSchRBCiBQj4SyEEEKkGAlnIYQQIsVIOAshhBApRsJZCCGESDESzkIIIUSKkXAWQgghUoyEsxBCCJFiJJyFEEKIFCPhLIQQQqQYS7IrIIQQQoxbWkMkAKE+CPdB2B+fD16PbxsFCWchhBAXj9YQ9mOO9ELAZ6wP7BwoM9zrEssxiIYgGoRoGCLx+eBt0dCZt0dDg5aH23bq/kHbI8F42PaeV+iOlISzEEKIoWIxiMRbfKHeU1qDg7aFeoxtob5By70D05nW0awC2JTk72m2xSfr0GWT9ZTtVrB6jGWLDaxpYHUak61/2TVoPmjZNmj9W8UjrpqEsxBCXCxaQ7AL/B0DU7DbaJnFIsaUWI5CLL4cje8bdj1qtCT7pyHr8eXYqevxeTQ8fOhG/KP/bv2hZEsbmKwucOUYAWVLM0LNlgY2F1XHTlJWVhZ/sRp4H6XOvU2pgTC12AYFq+3s2y02I3gtdjBZBr1v6pFwFkJMLFobYRTsMVp7wW5jXcdOL3f6i4d9v+zWd2BngxG2gc6h4esftB7oPP1zRkOZ4y07y6DJbGxXJjCZjLkyDdpmHrRt8LrZeL07b1Cwntr6O8M2qzO+fVAIm0bXv7g2UknZsrXnfywucRLOQojkiEaM63aJ0549A9f3+luRZ21NhoeuR4LxsO0ZOh9u23Ah+x7MBdjbv6bA4QGnF5yZxtw7BRzx5cQUX7enG626/rA9NXwHr6dwS0+MLQlnIcTIxKJGK/O0yTd0ffA1xv7QPXU93Gf0cB1rFifY3WCLT3a3cVrVO3XotsQ8feAUrMk8zBsOE4bDBOT23QdYtPJqI4AdnjO8lxAjJ+EsRCrROt4btO+U3qbB+HJ/j9HQoG3D7I9F4tcZB19jjA5caxyyHh0oG4syp6EGTv776SEc6hlSTWKgYwqt4/OYcbbU7Hai7P3XFt3G6U9HBmQUxtcH7xu0bHUNXBNMtBjjp177t5mGO63b38K0gXnkv9J0LEa0s5NIczPRpnbM2dlYi4owp6eP+p+t+3gEsqaP+nXvRay3l3BTEzoYRMdiENOAhljMWNfamGIxdEzHrznrgbI6ho5E0IEAMX+AWMCfWNYB/8A2f4BYYPC2ANrvR0ciWCeVYJ9eir2sFFt8bs7KQqVAC1+HQoSbW4g0NxFpbCTc1EykqYlIayvKbsOcmTlkspyyrqzWkX9WLEbU5yPa3k6krY1oeweR9jaibe1EO9qJtLUTbWsbVf0lnIUYTn9IJoIucoZOO+EzdNYJD/RmDfa3HOOtynOtxyJj+10S1xfNg+amIeuxqImeGkXXUQh0xuhSFnTMBBp01I6O2dGxbHRUo6MxiJ75uqklLw/n/Pk4F8zHOX8+jjlzMDmdY/udzkJrTay7m0hzM5HmZsLNzUSaWxLrkaYmwi3NRFpaIRw+7fUmtxtrUdGgqTCxbCkqwpKTgxrl9dXRigUCRqA0NhJuaCTSZMzDjQ1EGoztsa6uC/b5ym7H5HCgnM4hc5PTgdnrxeRwgMlE6ORJfH/+M7He3sRrzR4PtrIy7KWl2EunYystw146HUtBwZiEto5GifX0EG5qItLUTKS5yVhubDL+beP/xtH29tO/l8OBJScHHQoR7exEh0Jn/BxTWtppAW7OzASTiWhbG5GOdqJt7UTa24l2dEA0Ouz7mDMzMWdnY/F6R/U9JZzFxBAJQV/bMFM79LUOs61tVKddjY6vJqLBgSkWBQa3LJUNTDa0yT6wjBWt0tAqE7CgsaAxY3an4bpsGq450zE50+I9Te2Dep3az7wtcf2yv+PP8L8QdThM75tv0vXcerpffZVYby/m3Bz6SgrJKSxCWSwoqxVltUD/ssUan1tQVktimfhcBwL49+zFv2sX3a+8YnyQ2YyjoiIR1s7587FOmXLev6h1OEy4qYlwbS3hujpC8XmkvsEI3eYWtP/03sam9HQseXlY8nJJW3J5fNmYzN5Mou3thOvqCdfHp4YG+nbsOC0EldWKpXAgsK2Fhbiam2g/ceK8vk8sGBoI4nj4Rjs6TitnzsrCWlCAddIkXIsXYykswFpQYPzho1T8Dy5l/OEQX1cmFe+opQb2xfcrpcBsweQaGsDK4RjVHx9aayJNTQSrjhKqPkqw6ijB6qN0v/wynZ2dA8c/LQ3b9OnYS0uxlU7HWVdP29GjxPr88Ra631j2+4n5+9D9y4HAkHUdDA5bD3NWFpb8fKx5eTjnzsWSn4c1Px9Lfj6WvHys+XmYPJ7Ez53WGu33E+3sHDJFTlk3Jh+hmhqinZ0Qi2HOzsKSlY110iSc8+cn1s1ZWViyszBnZRvzzEzj/0e/3/12xMdV6WF7JF54FRUV+tChQ0n57PGqsrKStWvXJrsaoxLz+4m2tw/5C3zIL+VIyLi1JOAzerIGfBDoX/cZ+yLBQa3V8EDLNBpGR0PE/CFifUGifUFi/hDRvjBRf4iYP0qorwebCqNifjCBMul4p1U9sGx3oVwelMsDaR5UmheV5gVHBlF/jGhvmGhvkGhviGhviEi3n2hPgGh3nzF19RHt6TtD794RsFpRZrMReGYzWK3EfD50OIyy2XAtXkzaiuWkrViBfcaM99Rq07EY/p076XruObpeeJFoRwemjAzSr70Gz0034VqyhA0bN47Jz1mkvR3/rl3GtHMXgd27ifUZAzaYMzNxzJ+XCGvnvHmJ08k6GjVavIOCN1xblwjjcFPT0FaKyYSlIB9rYRHW/DwsefmDgjfX+AWdm4vJ5Tqv7xHt6RkI7Pp6Ig0NQ0I80tJy/v/2/V/B4zHqWViAtaAQa2EBloKhyya7/T19RjJE2tsJVlUROnqU4NFqgkerCFUdNY7ZIMrhwOR0opwOTE4XJqfTWHc5jXWHA5PLafwecbowudOw5uUZwZtv/HubbLYkfcuRU0pt11ovHlFZCefx43zCOdLaSt+2bQQPH8ack4OtuBhrcTHWoqLz/mU1nGhnB6H97xI8tI9g1WGC1ScJ1TYSbu06rWOssoDJojGZYyhzzFiOT8o8sGzsh1jMQixsNlqmYROxkMk42xw/6zzGHW/PSlmtmLOyMHu9mL2ZWLze+Okub3ybsd2cmWn8cukPXIsVZTECGHO81Wk2g9k8bAsy5vfTt20bvZs207tlC8EjRwAwZ2eTtny5EdbLlmPNzztnnbXWBA8domv9enzr1xOpb0A5HKRfdSUZN95I2qpVQ36xXag/AnU0SrDqKP5dOxOhHao6auxUCtu0aehIhHBDw2mnmy15ecbPbUkJ1uIibCUl8eVirAUFo7o+ONZ0KMQbr77KypUrz+v1ymq9qKf9U0G0u5vNr7/OyquvNv5gv8CXCVLFaMJZTmtfYsLNzfS9844xvf0OoerqM5Y1e72JoB6YF2HNy8Ka5cZsiw+YEOwBfzv0thJprCF07CTBmgaCDR2EmvsItkWI9A0EjDJpbBkRnBlhPJfFsHocxExOtLYT03ZiMSuxmBkdNROLqPjlWk04FEWHosR6wsSCIbQ/YLQeHQ7M6emYPBmYszIwZ6RjS8/AnJGBKSMdc3oGZk8GpnRjnym+bk5PZ9Obb7LyiiuMji/h8MA8bMyJhIduj0QG9mmNOdOTCF6LNxPlcl2Uzi4mpxP3qlW4V60y/l2bmundsiUxdf31rwDYy8uNsF65AtfixUN+yYdOnkwEcqjqKFgspK1YTt4XvoD7yqswu9Mu+PcYTJnNOCpm4KiYgfejHwUg2tWFf88e/Lt2EdizF+Wwk3HdtcbPY3EJ1pL4H5Ip3GpUNhva5cKckZHsqowb5vR0YhkZmNIu7s/geCLhPM6FGxsTQdz39tuE4te9TC4nzjmleJavI60sG3uejWh7G+GmFsLNHYRbfYTbewh3HCH49n56umPoU/ozmGwxrGkRrK4osZCJYJeFaNA8aL/ClusibXYmtkn52KdOwl5ahnVaBSojD9JyjVtL3uNp2Pd0GtfhMDpxjHPW/Dwyb72FzFtvQcdiBA8dMoJ682Y6fv972n/9a5TVinPRIpxz59L71lsEdu8GwLV4MVnf/Abp11036k4pF5o5IwP3ihW4V6xIdlWESCkSzmehYzGiHR1EWluN3p6trcS6u4xTkfFTk5jNRicZiwVlMRsdZwaftrRY4vssAx1sbDZjstuNudU6fACFA4kWK31t5DVtJPz8dnp3H6Rv/wn6qloItxudlkw2cOVFyFzQiys3hMMbRpmOQgQ4aEwmZcZqT4fMdMhPB1uGMQCC3Y22uYmGbIR7FOGuCOHOEOH2AKG2bsJtXZiy0klfWYZ9xmxsZeXYy0rHrPfl2UyU012joUwmHLNm4Zg1i+z77iMWCNC3bTu9m41T4G2/+AX22bPI+/KXyLjhBqyFhcmushBilCZkOMcCASItLURaWom0thjLra1EWlqItrQOrLe1nbF7/JgzK0xmhTLHOyqpGMoUjd/xYmyz9Zmp6jX+yUw2javIhHduOq6yHBzTilFp2eDKMkYdcvUvZw1ss7rO2HNXYfwwWICJdfVr/DM5HLhXrsC90mh9xgIB41YXIcS4dUmHs9aacF09gX37jGn/fgL79w97/xsmE5bsbMy5OVhyc7HPmoklJxdLbi6WnBwsecbcnJGBjsXQYeN+Vh2JT30+6KxH+xrRXQ3Q1YTuakF3N6N72tA9bcb1zZhCR1V8zAdlrGNDm51o5SCm7MY6VrS2oLUZHTOhY4o+FCXXXYtr2Wrss2ZLq1IMS4JZiPHvkglnrTXhmpqBEN63j8C+/UR9PqOAxYK9rAz32rXYpkwxQjcexJacHMxer3Ea+kz8HdB6BI5vhq5a8NWCrw666ox50De0vDKBuwAmF0PGAvCUQEaxMci8K8sYUrC/dWsdWVu1srKSeePsViohhBCjNy7DWcdihE+exB8P4P5AjnV3GwWsVhzl5aRfey2OObNxzJmDfcaMc/f41Bq66qH1ELQchtb41HIIepuHlnVlG2HrnQpTVxrL/QHsKYb0QmOYQSGEEGKUxlU4B48do+M3v8X33HOJUXuUzYa9ooKMG2/AMTsexOXlZ78hPRqG9mPx8O0P4kNGy3jQ+MHYPZA7A8qvNeY5MyC7zAhg29jdIyyEEEIMlvLhrLWmd/MW2n/zJL0b3kBZraSvu560pUuNIC4tPfcABF0NcHILnNgCJ980WsKxQYMcZBQbwbvg4/EQrjDW3XnyiDYhhBAXXcqGc6yvD9+zz9L+m98SOnoUc04OOQ8+gPe227Dk5Jz5hVpDezWc2Awnthqh3HHc2Gdzw6TL4y3heADnlBu3EwkhhBApIuXCOVxfT8dTT9Hxh/8l5vPhmDOHou/9G+nr1g1/qjoWhaZ98VbxFiOQ+68Pu7Jh8jK4/H6Yshzy547qkXJCCCFEMqREUmmt8e/YQfuTv6H71VcBSL/mGrLuuhPnwoVDB7rQGmq3wfE3jCCuecsYYhLAMxlKrzQCecoKo1Usp6WFEEKMM0kN51goRPcLL9D+5G8I7NuHyeMh+5P34L3jDqxFRcO/6G/fhk0/MJZzZ8JlHzJaxZOXQeaki1d5IYQQ4gJJWjibfD6qrrqaaGsrttJSCr75TTw3f+DsT0ra+H0jmN93F1z9TUjLvmj1FUIIIS6WpIWzudOHY85ssu68i7QVy889RvNbjxut5rkfgZv+w3iQvBBCCHEJSlo4R4qLmPzzn4+s8M6n4IUvQ8WNcMtjEsxCCCEuaUkbnFlbRvh3wf6/wF8+C9PXwoefkFG3hBBCXPJS+8kJR16B/70PSpbAx54CqwzoL4QQ4tI3onBWSl2vlDqklKpSSn1lmP2TlVKvK6XeVUrtVkrd8J5rdnwTPP0JyJsFdzwDtrT3/JZCCCHEeHDOcFZKmYGfAOuA2cDtSqnZpxT7GvCM1noh8DHgp++pVnXb4amPQeZkuPNP4Mx8T28nhBBCjCcjaTlfDlRprau11iHgf4APnlJGAxnxZQ9Qf941atoPv/2Q8SjFu/4CaWcZqlMIIYS4BCmt9dkLKPVh4Hqt9afi63cCS7XWDwwqUwi8DHiBNOD9Wuvtw7zX/cD9ALm5uYueeeaZIfudffUsfPeraGXi3YUPE3AWvKcvd6np6enB7XYnuxrjihyz0ZNjNnpyzEZvIh6zK6+8crvWevFIyo6ky/RwNyCfmui3A7/SWn9fKbUM+I1S6jKtdWzIi7R+HHgcoKKiQq9du3Zgp68WnngArGa453muyJs5kvpPKJWVlQw5ZuKc5JiNnhyz0ZNjNnpyzM5uJKe1a4HB42KWcPpp6/uAZwC01lsBBzDy89E9zfDkByHgg0/8ESSYhRBCTGAjCed3gHKl1DSllA2jw9ezp5Q5CVwNoJSahRHOLSOqQV87PHkLdNXDx/8ARQtGXHkhhBDiUnTOcNZaR4AHgJeAAxi9svcppb6tlLo5XuwfgU8rpXYBvwfu0ee6mA0Q7IbffQTajsDHfgeTrzjvLyKEEEJcKkY0TJfW+nng+VO2fX3Q8n5gxWg+WKHh97dD/btw22+g9KrRvFwIIYS4ZCVtbG2HvxGON8PfPQ4zb0xWNYQQQoiUk7RwtkR64aZfwLyPJqsKQgghREpK2tjaQXsOLL43WR8vhBBCpKykhXPIJkNyCiGEEMNJ7adSCSGEEBOQhLMQQgiRYiSchRBCiBQj4SyEEEKkGAlnIYQQIsVIOAshhBApRsJZCCGESDESzkIIIUSKkXAWQgghUoyEsxBCCJFiJJyFEEKIFCPhLIQQQqQYCWchhBAixUg4CyGEEClGwlkIIYRIMRLOQgghRIqRcBZCCCFSjISzEEIIkWKSFs7hWLI+WQghhEhtSQvn5j5JZyGEEGI4SW05N3cFkvXxQgghRMpK6jXnTVWtyfx4IYQQIiUlLZxNCjYdkXAWQgghTpW0cHaaFRurWtFaJ6sKQgghREpKXjhboaU7yKGm7mRVQQghhEhJSQtnh1kBcmpbCCGEOFXSwtligtLcNDZKOAshhBBDJLW39qryXN461kYgHE1mNYQQQoiUkuRwziEQjrHjREcyqyGEEEKklKSG89Lp2VhMRq9tIYQQQhiSGs5uu4X3TfZKpzAhhBBikKQ/lWpleQ57632094aSXRUhhBAiJSQ9nFeV56A1bJZT20IIIQSQAuE8rySTDIdFTm0LIYQQcUkPZ7NJsbw0h41HWmQoTyGEEIIUCGcwrjvX+wJUt/YmuypCCCFE0qVEOK8uzwVkKE8hhBACUiScJ2e7mJzlkqE8hRBCCFIknME4tf1mdRvhaCzZVRFCCCGSKmXCeVVZDj3BCDtrOpNdFSGEECKpUiacl5fmYFLIqW0hhBAT3ojCWSl1vVLqkFKqSin1lTOU+ahSar9Sap9S6qnRVsTjsjKvJJONR1pG+1IhhBDiknLOcFZKmYGfAOuA2cDtSqnZp5QpB74KrNBazwH+4Xwqs6o8h101nfj84fN5uRBCCHFJGEnL+XKgSmtdrbUOAf8DfPCUMp8GfqK17gDQWjefT2VWluUQ07D1aNv5vFwIIYS4JIwknIuBmkHrtfFtg80AZiilNiul3lRKXX8+lVk42UuazcymKjm1LYQQYuKyjKCMGmbbqeNsWoByYC1QAmxUSl2mtR7S9VopdT9wP0Bubi6VlZWnvXGZB17eXcP7M6X1fKqenp5hj5k4MzlmoyfHbPTkmI2eHLOzG0k41wKTBq2XAPXDlHlTax0GjimlDmGE9TuDC2mtHwceB6ioqNBr16497cOOWY/xrb/up3Te5UzKco30e0wIlZWVDHfMxJnJMRs9OWajJ8ds9OSYnd1ITmu/A5QrpaYppWzAx4BnTynzZ+BKAKVUDsZp7urzqdCq+FCeckuVEEKIieqc4ay1jgAPAC8BB4BntNb7lFLfVkrdHC/2EtCmlNoPvA58WWt9XuelS3PTKPQ45LqzEEKICWskp7XRWj8PPH/Ktq8PWtbAF+PTe6KUYmVZDi/vbyIa05hNw13yFkIIIS5dKTNC2GAry3Pw+cPsqfMluypCCCHERZea4VyWA8AmGS1MCCHEBJSS4ZzttjOnKIM3pFOYEEKICSglwxmMU9vvnuygNxhJdlWEEEKIiyplw3lVWS7hqOatYzIYiRBCiIklZcN58VQvdotJ7ncWQggx4aRsODusZi6fliXhLIQQYsJJ2XAG4xGSVc09NPj8ya6KEEIIcdGkdDivLDOG8twkrWchhBATSEqH88yCdHLcdjm1LYQQYkJJ6XA2mRQry7LZXNVKLHbqUyqFEEKIS1NKhzPAyvJc2npDHGjsSnZVhBBCiIsi9cM5MZSnnNoWQggxMaR8OBd4HMzId8t1ZyGEEBNGyoczGL223z7eTiAcTXZVhBBCiAsuaeHcHe0ecdlV5TmEIjHeOd5+AWskhBBCpIakhXNntJPXTr42orJLp2dhNSu57iyEEGJCSFo4W5WVr2z8CofaD52zrMtmYdEUrzxCUgghxISQtHDOteSSbkvngdceoNV/7tBdVZ7LgYYuWrqDF6F2QgghRPIkLZzNysyPr/oxnYFOPv/65wlGzx66/bdUbTkqrWchhBCXtqT21p6dPZvvrvouu1t2840t30DrM48Cdlmxh0yXlTcOSzgLIYS4tCX9VqprplzDgwsfZH31ev5rz3+dsZzZpFhRmsOmqpazhrgQQggx3iU9nAE+PffT3DDtBn707o949cSrZyy3sjyHpq4gVc09F7F2QgghxMWVEuGslOLbK77NvJx5/Mumf2F/2/5hy/Vfd5bRwoQQQlzKUiKcAexmOz+86od47B4efO1BWvpaTiszKcvFtJw0Nh45fZ8QQghxqUiZcAbIcebwn1f9J92hbj7/+ucJRAKnlVlZlsNbx9oJRWJJqKEQQghx4aVUOANUZFXw8KqH2du6l69v/vppnb+unJlLXyjKg7/fga8vnKRaCiGEEBdOyoUzwNWTr+bz7/s8Lxx/gZ/v/vmQfVdW5PEvN8zkbweaueFHG9l+oiNJtRRCCCEujJQMZ4B7L7uXm0tv5ic7f8JLx19KbFdKcf/qUv7w98tQCj768608VnmUWExurxJCCHFpSNlwVkrxjWXfYGHeQr626Wvsa903ZP/CyV7Wf24V188p4HsvHuSeX71Da48M7SmEEGL8S9lwBrCZbfz72n8ny5HF5177HE29TUP2e5xW/vOOhfx/t17GW9VtrPvhRrZUyW1WQgghxreUDmeAbGc2P776x/SEe/jc65/DH/EP2a+U4uNLp/CXB1aQ4bDw8V++xfdfPkQkKr25hRBCjE8pH84AM7wzeGT1IxxoO8BDmx4ipk8P3pkFGfz1wZV8+H0l/Pi1Ku74xVs0+PzDvJsQQgiR2sZFOAOsmbSGf1z8j7xy4hUe2/XYsGVcNguPfmQ+/3HbAvbV+1j3w428ur9p2LJCCCFEqho34Qxw1+y7uLXsVn6262c8tvOxYVvQALcsLOa5z62iONPJp57cxrf/up9gJHqRayuEEEKcn3EVzkop/s8V/4ebS2/mp7t+ypc3fPm0a9D9puWk8cfPLOee5VN5YvMxPvzYVo639l7kGgshhBCjN67CGcBqtvKvK/6Vf1xknOK++4W7aextHLas3WLmmzfP4ed3LuJkex83/XgTz+6qv8g1FkIIIUZn3IUzGC3oey67hx9f9WNOdJ3g9vW3s7tl9xnLXzengOc/v4qKgnQ+9/t3+fvfbGdfve8i1lgIIYQYuXEZzv3WTFrDb2/4LXaznU+++Emeq37ujGWLM538z/1X8I/XzGDz0VZu/NEm7vvVO7x7Uob/FEIIkVrGdTgDlHvL+f2Nv2du7ly+uvGr/HDHD8/YUcxqNvHg1eVs+uer+NK1M9hxsoNbf7qFO3/5Fm9Vt13kmgshhBDDG/fhDOB1ePnFNb/gQ+Uf4r/2/Bf/8Po/0BfuO2N5j9PKA1cZIf0vN8zkQEM3tz3+Jh/92VbeONxy2pOwhBBCiIvpkghnMDqKfWPZN/jK5V9hQ+0G7nzhTup7zt75K81u4f7VpWz65yv51s1zqOno464n3uaWn27h1f1NEtJCCCGS4pIJZ4gP5Tnr4zx29WM09DRw+/rb2dG045yvc1jN3L18KpVfXsvDfzeX9t4gn3pyGzf8aBPrdzfIE6+EEEJcVJdUOPdbXryc3934O9Jt6dz38n386cifRvQ6u8XM7ZdP5vV/XMsPPjqfYCTKZ5/awbX/8QZ/erdWxusWQghxUVyS4QwwzTON393wOxazT9dBAAAgAElEQVTnL+brW77Oo+88SjQ2slHCLGYTf/e+El75whr+846FWEyKLzy9i6u+v4HfvHmC9t7QBa69EEKIiWxE4ayUul4pdUgpVaWU+spZyn1YKaWVUovHrornz2P38Nj7H+OOmXfw5P4neeC1B+gOdY/49WaT4qZ5RTz/uVU8fuciMl1W/s+f97L4X1/h9sff5Mmtx2n0BS7cFxBCCDEhWc5VQCllBn4CXAPUAu8opZ7VWu8/pVw68DngrQtR0fNlMVn46tKvUppZysNvPcwnnv8E31j2DeblzsNiOufXB8BkUlw7p4BrZuezr76LF/c28uK+Rr7+l318/S/7eN/kTK6/rIDr5xQyOdt1gb+REEKIS91I0ulyoEprXQ2glPof4IPA/lPKfQd4BPjSmNZwjHy04qNM80zjC5Vf4O4X7ybdms7SwqUsK1rGsqJlTEqfdM73UEpxWbGHy4o9fOm6CqqauxNB/d3nD/Ld5w8yuzCD6y8rYN1lBZTluVFKXYRvJ4QQ4lIyknAuBmoGrdcCSwcXUEotBCZprZ9TSqVkOAMsKVjC+lvXs7VhK1vrt7KlfguvnnwVgEnpk1hWuIzlRctZUriEDFvGOd+vLC+dB65K54Gryqlp7+OlfY28sLeRH7xymB+8cpjpuWmsi7eoLyvOkKAWQggxIupc9/IqpT4CXKe1/lR8/U7gcq31g/F1E/AacI/W+rhSqhL4ktZ62zDvdT9wP0Bubu6iZ555Ziy/y6hprWmONHPQf5CDgYMcCRwhqIMoFFPtU5npmMlMx0ym2KdgVuYRv29HIMaO5ijbmyIcbI8R05DtUCzON7Mgz0K514TFdOagjukY3dFu2iJttEfbaY8YU0Ysg2tzrsWiRnY6XkBPTw9utzvZ1RhX5JiNnhyz0ZuIx+zKK6/crrUeUZ+skYTzMuCbWuvr4utfBdBaPxxf9wBHgZ74SwqAduDm4QK6X0VFhT506NBI6njRhKNhdrfuZkv9FrbWb2Vf2z5iOobb6ubygstZVrSM+bnzcVqc2Mw2bGYbVpMVq8mKzWwb9hp2e2+IVw808dLeRjYeaSUUjZHuUFwxw8rsSREKsv10hpto6Gmgvree+p56GnsbCcfCQ94nw5ZBV6iLcm8531n+HebkzLlYh2Vcq6ysZO3atcmuxrgix2z05JiN3kQ8ZkqpEYfzSJpg7wDlSqlpQB3wMeCO/p1aax+QM+jDKzlDyznVWc1WFuUvYlH+Ih5c+CC+oI+3Gt5KhPVrNa+d9fUmZcJmsmE1W7GZbEMDPNvKwmwz9T3N+EKtvBmO8WY1UG281mX2UuIuYk72HN4/5f0UpxVT6C6kKK2IIncRLquLHz//Y/7c82fueP4O7plzD59Z8BnsZvuFPzBCCCEuqnOGs9Y6opR6AHgJMANPaK33KaW+DWzTWj97oSuZLB67h2unXsu1U69Fa83J7pMc7jhMMBokHA0TjoUJRUOEYiFjHg2dti0cCxOOhgnFQkRiEaZnTqUwzQhdf8BDVZ2Fd6o0++v9NAFNXidXz8xj0ax8lhZkYbcMnE6f65rL3e+/m+9v+z5P7H2C106+xndWfIcFeQuSd5CEEEKMuRFdvNRaPw88f8q2r5+h7Nr3Xq3Uo5RiSsYUpmRMGds3nm/MGn0BXjvYzGsHm3h6Ww2/3noCl83MqvIcrp6Zz9qZuYBxevtby7/FdVOu41tbv8VdL9zFx2d9nAcXPojLKrdxCSHEpUB6FqWIAo+DO5ZO5o6lkwmEo2w92sbfDjbx2oFmXtrXBMDkdBNXde1jydQslkx7H3/84B/5j+3/wW8P/JbKmkq+ufybLC1ceo5PEkIIkeoknFOQw2rmypl5XDkzD/1BzcHGbv52oIn126p4+p0afrXlOABTs10smXoTd01dyCvNP+ZTL3+Kj8z4CF9c9EXctonVC1IIIS4lEs4pTinFrMIMZhVmcJmpjhWrVrO3zsc7x9t5+1gHrxxoonO7BnU/mcWv8YfD/8sL1a/z2bn/zO2XXYf5LLdsCSGESE0SzuOM1Wxi4WQvCyd7uX81xGKaqpYe3j7WzjvHp/Jm7QJ8GU/xvXe/zPfeeJr5aXeybOpklkzNYnZRBm77uf/JfUEfNd011HbXUtNdQ11PHdM807hh2g3kunIvwrcUQoiJTcJ5nDOZFDPy05mRn84nrpgCLKS69UP84J2f8gZPsyd2iDe3fJDIS5cBMDnLRUV+GpPzwngzu7E7OuiJNlHbU0NtjxHGpz4cJNOeSWewkx9s/wHLCpfxgdIPcNXkq3BanEn4xkIIcemTcL4ETc/J5D/X/QsH2m7l61u+zkHTb6nIWERfSNMaqOdN3cqbzRFoNsprbcKmc/DaCih3r2bGpKksLCylLGsKJe4SXFYXx3zH+OvRv/Jc9XN8ZeNXSLOmcc2Ua7i59GYW5S/CpC7Zp48KIcRFJ+F8CZuVPYunbnyK/9773zx96GmyXdnMzJ9LSXoJBc5idDibrm4P9a02DjX2ceBEF0cDETYASoWYklXHrMJuZhZkMKcog9vK7uezCz7LjuYdPHv0WV4+/jJ/rvozhWmF3DT9Jj5Q+gGmeaYl+2sLIcS4J+F8ibOarNw/737un3f/Octqranr9HOgoZsDDV0caOjiYGM3L+5rpH+U1xy3nbnFGVxW/Am+Nvc+fKadbGp4kV/u/SW/2PML5uXM46bSm1g3dR2ZjswL/O1Ev5iOsatlFy8df4ntTdtZN20dd866E6vZmuyqCSHOg4SzSFBKUeJ1UeJ1cc3s/MT23mCEAw1d7K3zsafOmG843EJMA1jJSvsQs4puxZG5m7qejXz3re/yyDuPsLp4NTeX3syyomVEdAR/2I8/4qcv0oc/4h8y9YX7hl0PxUKUekpZkLeAuTlz5RaxQbTW7G3dy4vHX+TlEy/T2NuIzWSjNLOUf9/+7/yl6i88tPQhLi+8PNlVFUKMkoSzOKc0u4XFU7NYPDUrsc0finKgsYt9dT721PnYW9fFtqNziMRmY7LXk5a9izei2845HvlwHGYHTosTp8WJ2WTm5eMvo9GYlInyzHIW5C1gfu58FuQtoMRdMqEexam15mD7QV48/iIvHX+Jup46LCYLK4pW8LmFn+PKSVfitrnZULOBh99+mPtevo9109bxpcVfIs+Vl+zqCyFGSMJZnBenzcz7Jnt532RvYlsgHOVwU3c8rK9gT107VV07iNlq0dqKitnISUunKMPDJG8m07K8lOVmMSM3m0ynG5fFhcPiOK1zWU+oh92tu9nVvIudLTt5rvo5nj70NAA5zhwW5C5IBPbs7NnYzLZRf59gNEhHoGNgCnYQiAQozSxlhndG0odGPdJxJBHIJ7pOYFEWlhYt5e/n/z1XTb7qtOePr5m0hqWFS3li7xP8cs8v2VCzgc8s+Ax3zLoDq0lOdQuR6iScxZhxWM3MK8lkXsnAteZQZBVHW3qoau7hSFM3R5p7ONLcw85DvURiIaARpRqZnOWiPC+d8nw35XluZuSnU5rrxmkz47a5WV60nOVFywGIxqJUdVaxq2UXO5t3srNlJ6+efBUwrrHPyZ7DgrwFLMhdwInACWInY3QGO2kPtNMZ6KQj2DFkuSPQQV+k74zfS6GY6pnKzKyZzMqalZhf6Gvqx3zHjEA+9hJHfUcxKRNLCpZwz5x7eP/k95/z8x0WB59Z8Blumn4TD7/9MP932//lz1V/5qGlD7G4YERPrbtoorEohzoOUZRWJH0VhEDCWVxgNospMcLZYKFIjONtvRxp6uFIc3divuFwM+Go0ftMKSjxOinJdFGY6aA400mhxxlfLmLdlOl8tOKjALT6WwfCunknvzvwO36171fGhzUNfK7T4iTTnonX4cVr9zLVMzWxPGTu8GIxWajqqOJg+0EOtB/g3eZ3eeHYC4n3KkwrHBrY2bPId+Wf8zR7TMfoDnXTGeykI9AxdB7soDPQyf62/RzqOIRCsSh/EQ/NfIj3T3k/Oc6cs773cCZnTOanV/+U12te53tvf49PvvRJPjD9A3xx8RfP6/3GSneomy31W9hQs4GNdRvpDHZiURZWFK/gxuk3snbSWrmXXkxYEs4iKWwWU2LwFChMbA9HY5xIhLbR4q7r9PPm0TYauwLxTmgD0u0WijKNwC705FHkuZlb8m/jU+Um/JzkyOHtXHXF2kTojvaX/aT0SVw5+crEemegkwPtBxKBfaDtAJU1lWiMinntXiqyKpiVNQuzyXxa+HYGO/EFfUR1dNjPs5qseB1eStwlfOXyr3DNlGvG5FqxUoqrJl/FsqJl/GL3L/jvff/N6zWv88DCB7it4jYspovzq6Cmq4YNtRuorK1ke+N2IjqCx+5hVfEqlhct50jHEdYfW8+G2g24LC6unnw1N06/kaWFSy9aHYVIBUprfe5SF0BFRYU+dOhQUj57vKqsrGTt2rXJrkbSRKIxmruDNPj81HUGaOj00+ALUN/pp97np6EzQFtvaMhrTAqmZqdRmuemLM9NWa6b0jw3pblppDvG5tprX7iPwx2HB0K77QBHOo+gtU600gfPT93mtXvJdGTitRt/PFyMDm7Hfcf57lvfZWvDVmZmzeShpQ8lngs+lj9nkViEXS272FCzgQ21G6j2VQNQ6ill9aTVrC1Zy7zceUOCN6ZjbG/azvrq9bx84mW6Q91kObJYN20dN0y7gbk5c1OuE+BE/795PibiMVNKbddaj+iakoTzODIRf5hHKxCO0uAzgrveF2DDjv1EnNlUNfdwvK03ccocoCDDYQR2nhHYRnCnkeu2v+df/tFYFJMypVyIDKa15pUTr/DIO4/Q1NfELWW38IVFX2D3m7vf089ZV6iLzXWb2VC7gU11m/AFfVhMFpbkL2HNpDWsLlnNpPRJI3qvUDTExtqNRmu6ZgOhWIjJ6ZO5YfoN3DjtRqZ6pp53PceCL+hja/1W3trzFnNmzsFutmM323FYHNjMNhzmU+aDtltN1pT++bjQJuLvMwnnS9RE/GF+rwYfs3A0xsn2Pqrip8uPNvckOqv1hgZOM2c4LEZg57qZnOWiJMvJpPj933npdkyX2JO++sJ9/Gz3z/jNvt/gtDqZY51DUVERMR1Da504Zd+/rNHG8qnraNoD7exs3klUR/HavawqWcXaSWtZVrjsPd+j3h3q5tUTr7L+2HrebngbjWZO9hxunH4j66atuyjXz7XWHPMdY0OtcSag/7ueD4XCbraTZk1jSsYUpmdOZ1rGNKZnTme6ZzoFaQWX5LC4oWiIQ+2H2PPuHj52zccuye94JhLOlygJ59EbyTHTWtPYFUiEdv9U3dpLS3dwSFmbxURJppNir5NJWS5KvEZw9y9np9nGbWuourOaR7Y9wt7GvdjtdhQKpRQKlfgFOnjbaXMULquLKwqvYHXJaubmzMVsMl+Qujb3NfPCsRdYX72eA+0HMCkTC3IXMDt7NjOzZjIzaybTM6ePyW1joWiIbU3beKP2DTbUbKC2pxaACm8Fq0tWs2bSGmp217D4isUEo0FjigQHlqNBQtEQgWjAmEcChGLxeTSEL+TjuO841b5qOoOdic91WpxMzZjKNM80pnmmMd1jhPbkjMnndbtgsjT2NrKrZVdiOtB2gHAsDIDb6mZe7jwW5C5gft585uXMu6QHGpJwvkRJOI/eez1mgXCU2g4/tR191MTnte1+ajr6qO3w037KNW6n1WwEdpaLAo+DvHQ7eekO8jOMeV6Gnew0GxZz6rYWxtvPWXVnNeuPrefNhjc50nEEf8QPGJ3ryjLLmJU9iwpvRWI+knvWW/2tbKzdyBu1b7Clfgt9kT7sZjtLC5eypsQ4NV+QVpAoP1bHrD3QzjHfMap91VR3VnOs6xjHOo9R31ufKGNWZkrSS4aEdqmnlGmeaUkPtlA0xP62/UPCuLnPeMKO3WxnTvYc5uXOY17uPN7d8y7B7CA7W3ZypOMIGo1CUeYtGzJ2weT0ySnxB29DTwOb6zfTE+phTs4c5mTPGfX4B6MJZ+n+KMRZOKzmxHXp4fQEI9R1+Klp7xsS4DXtfnbVdJ7WQQ2MTmpZafZ4YA+Edl66nbwMI9CNYHdgvsROoV8I0zOn8+DCB3lw4YNEY1FOdJ/gYNtBDrYb0+snX+ePR/4IGC3/KRlTqMiqSNwGV5FVQbYjm4PtB9lQu4E3at9gT+seAPJd+dw0/SbWTFrDkoIlF/zWrixHFlmOLBblLxqyvS/cx4muE0Zo+6o55jvGMd8xNtVtIhKLJMrlufISLezSzNJEeGc5si5IwDX2NrKzZSe7mnexu2U3B9oHWsXF7mIW5S9ifu585ufOp8JbMWSsd+sxK2uXrQWMgYb2tO5JvNcLx17gD4f/ABh3QMzPm2+0rnPnMydnzkW5xS4YDbK9cTub6jexuW5zojNjP5MyMd0znbk5c7ks5zLm5sylzFs2ZoP8SDgL8R647RYqCtKpKEgfdn84GqO1J0hTV5DmrgDN3UGau4O0dAeMbd0B9tV30doTPO02MYtJJe7vLs50Uex1Jk6pF8dvH7NbLsxp4/HKbDInwumG6TcAxmWLpr6mRFgfbD/I3ta9vHT8pcTrnBYn/ogfhWJu7lweXPgga0rWMMM7IyVabS6ri1nZs5iVPWvI9kgsQm13bSK0qzuN+Z+q/pQ4gwDgsXsSx2W6Z3riurbX4aU33EtvuJeeUA894fgUGpj3hnuHbOsN99Id7qbd305boA0YaBV/YtYnmJ87n3m588h15Y74+7ltbpYVLWNZ0TLA6LFf3VnNzhZj3IJdLbuorKkEwKIslHvLKfeWU5ZZlpgK0gre07+V1prjXcfZXLeZzfWb2da4jUA0gM1kY1H+Iv6u/O9YWbwSr8PL3ta97G3dy57WPbxe8zp/qvpT4jjMypqVCOu5OcZTAM+nXhLOQlxAVrPJGDjFc/a/9KMxTVtvkOZ4YDf6gtR1GqfO6zr8bDnaStMp93krBbluOyVeJ8VelxHi8QAv8DjIz3DgdU3sHsFg3ONdkFZAQVoBayetTWzvCnVxqP0QB9oOcLL7JHNz5rKyeCXZzuzkVXaULCYLUz1TmeqZylVcldje/wfJ0c6jQ4L7byf/xv8L/r9RfYbNZMNtc+O2uhPzEncJc7LnMDNrJgtyFzAja8aYDgtrUibKvGWUecv48IwPA9AR6GB3y252tuxkb+tettZv5dmjzyZek2ZNozSzlLLMMko9pcbrM8vIdeae8f9Ab7iXtxreSgRyXU8dAFMzpvKhGR9iRdEKFhcsPq2lvrpkNatLVgPGsa7tqU2E9d7Wvfzh8B/47YHfAsYfRv1hPRoSzkKkALNJGae30x2AZ9gy4WiMRl/ACOxOI7RrO/qo6/Szu7aTF/c2DLlVDIwObPkZdgoyjLDun+d7jOWCDOOUusM68VrgGbYMlhQsYUnBkmRXZcwN/oNkRfGKIfvaA+2JFnZXqAu31U2aNW1I+PYvp1nTUqbzmdfhZc2kNayZtCaxzRf0UdVZxdHOo1R1VlHVWUVlTWXiMgZAui2d8sxySjNLKc0sZUrGFA62H2Rz3WZ2Nu8koiO4LC6WFi7l3svuZXnRckrSS0ZcL6UUk9InMSl9EuumrQMgHAtztPNoIqz3tO7h8d2Pj+r7SjgLMU5YzSYmZRk9w4cTi2laeoLUdvhp6grQ6AsY8/jyvvou/nagGX/49Ft/vC6rEdoZDnRfkB2hQ+T1B3mGnfwMR8p3ZBMjk+XIIqsgK+XGVz8fHruHRfmLTrtG3+ZvSwR2//yl4y/RFepKlJmZNZO759zNiuIVLMhdMKbPPrearIm7Bj4y4yOA0W8g7e60Eb+HhLMQlwiTSSUC9ky01nQFIonwbuwK0NQ/7wrS1BWgpjXKprqq066BmxTkuO2JwM7LcJCfPhDeeYNCfKKfShfJle3MJtuZPeRZ5lprWv2tHO86ztSMqaO6Jj4WRtuzW8JZiAlEKYXHacXjtMbHNT9dZWUlK1etpq03RNOg0G7uX+4OUNcZ4N2Tw/dGt5lNFHgcFHocxrjnHgeFmU6KPI749XcHmXItXFxkSilyXbkXPZTPl4SzEOI0FrPpnK1wMJ4u1tIzEN4NPqNFXh8fQvXtY+00dgWIntIMd1rN8dA2ArsoHuDZaTYyXTYyXVYynVY8Lqv0SBcTkoSzEOK82Sym+K1eZ+6NHo1pWuIPLOl/UEmDL0CDz099Z4CNR1po7g5ypvGQnFYzmS6jtW+EthHenkHLXpcVr8tGbvxe8TSbWVrmYlyTcBZCXFBmk6LA46DA42DhGcqE408c6+gN0dkXptNvzH3+MJ19ITr6wvH1EEdbeuj0h/H1hQlFY8O+n9NqTgzskhsf6CU3sTywnp1mu+TGSheXBglnIUTSWc3nboGfSmuNPxw1wrwvTHtviObuAC3xgV6au42BXw42drPxcCvdwchp72E2KXLctkSA58XDOzdjYDkvw0Gu247NIj3VxcUj4SyEGJeUUrhsFlw2C0UjCHV/KBoP7kB8lLb4clcw/pzwALtrfbT1Dn+K3euyJoZazXXbyc0YCPT69ijT2nrJz3BMyHvGxdiTcBZCTAhOm5nJ2S4mZ5/9lpZINEZbbygxWpvRAh8a6tUtvTR3B4YM+vLw25UAeJzWgdvL0h0UeAaW+7fnptuxyj3j4iwknIUQYpChPdWHH60NjNPqnX1hmruDvLLpbfKnzqC5Oxi//cy47ayquZXm7uBpvdWVguz4w09y0+1kOq1kxG9x8wxaznDEt8U7xElHt4lDwlkIIc6DUgpvmg1vmo2GHDNrF08atlwspgfdMz7ovvFu47azlp4gR1t66PJH6AqEz9hrHYxr5BkOy5AQPzXIM5yWQcv9+yxkOK3SWh9HUiqcw+EwtbW1BAKBZFclJXk8Hg4cOIDD4aCkpASrdeyGmxNCXBgmk0r0FL+s+MwtcTCCvDsYoctv9FTvn/v8YboCg5b9kcRyXac/Ue7UsdVP5bKZh4S4x2kjx20jK81GttvovW4s28hOs5OVZpOOcEmSUuFcW1tLeno6U6dOlVM3w+ju7sbtdtPW1kZtbS3Tpk1LdpWEEGPIZBoYwW34dviZaa0JhGN0BQZCvT/Q+8N88PYuf4Tajj521XbS0Rsicup4rXHpDgs5biOos9IGhXmanWy3jVy3nWy3nRy3Da9Lbk0bKykVzoFAQIL5HJRSZGdn09LSkuyqCCFSiFIKp82M02Y+58hup4rFNF2BMG29Idp7Q7T1BGnrDdHWY6y39gRp7w1xsq2Pd0920t57+vPHwRh/PSvNCOoc98C8P7xz0u3kpNnJSbcROkcrf6JLqXAGJJhHQI6REGIsmUwqPmyqjdIRDD0di2k6/WHaeoK09hjhPXi5f368rZfWniCB8PCDxTgqXxgY8e0sI8D1n03IdFnJdNkmRMe4lAvnZHO73fT09CS7GkIIkbJMJpU4zV2ef+7yvcEIbT0hWgaF+I69B8kqnERnX/+ocGGOt/bR6e+koy9MKDJ8oANYTIpMl9HhLTMR3LZTQrw/1G2JbZ5x1ClOwlkIIcQFlWa3kGa3DLnHvMhfzdq1s874mkD/6G/+EL54ePvi6x19A53jfH1hWnqCVLX00NkXpjtw+khwQ+piM+N2WHDbjSnNfspyfF+azUya3UK6w5Kof7rd6PWeeREeyCLhfAZaa/7pn/6JF154AaUUX/va17jttttoaGjgtttuo6uri0gkwmOPPcby5cu577772LZtG0op7r33Xr7whS8k+ysIIcS45bCaKfCYKfCM7vp5NKbp8sfDPD42uy+xbMx7AhF6gsbUG4xwsrcvsdwTjJyz1zuA224hK34rXZbLijfNRnZi3Xbausc5urtrUjacv/XXfeyv7xrT95xdlME3PjBnRGX/+Mc/snPnTnbt2kVraytLlixh9erVPPXUU1x33XU89NBDRKNR+vr62LlzJ3V1dezduxeAzs7OMa23EEKIkTGbBu4/P1/BSJTeYJTeYITuQITeUDzMA5FE4Lf1hujoDdEeb7kfbuqhvTeEPxwd9j1H24k9ZcM52TZt2sTtt9+O2WwmPz+fNWvW8M4777BkyRLuvfdewuEwt9xyCwsWLGD69OlUV1fz4IMPcuONN3Lttdcmu/pCCCHOk91ixm4xk3UeAe8PRWnvM4I7EeDx6cv/NvL3SdlwHmkL90LRZximZ/Xq1bzxxhusX7+eO++8ky9/+cvcdddd7Nq1i5deeomf/OQnPPPMMzzxxBMXucZCCCGSzWkzU2wb/glrXx7F+4yPbmtJsHr1ap5++mmi0SgtLS288cYbXH755Zw4cYK8vDw+/elPc99997Fjxw5aW1uJxWJ86EMf4jvf+Q47duxIdvWFEEKMYynbck62W2+9la1btzJ//nyUUjzyyCMUFBTw61//mkcffRSr1Yrb7ebJJ5+krq6OT37yk8RiRtf/hx9+OMm1F0IIMZ6NKJyVUtcDPwTMwH9prf/tlP1fBD4FRIAW4F6t9YkxrutF0X+Ps1KKRx99lEcffXTI/rvvvpu77777tNdJa1kIIcRYOedpbaWUGfgJsA6YDdyulJp9SrF3gcVa63nA/wKPjHVFhRBCiIliJNecLweqtNbVWusQ8D/ABwcX0Fq/rrXui6++CZSMbTWFEEKIiWMkp7WLgZpB67XA0rOUvw94YbgdSqn7gfsBcnNzqaysHLLf4/HQ3d09gipNTNFoNHF8AoHAacdPnK6np0eO0yjJMRs9OWajJ8fs7EYSzsPdOj3sfUZKqU8Ai4E1w+3XWj8OPA5QUVGh165dO2T/gQMHSE9PH0GVJqbu7u7E8XE4HCxcuDDJNUp9lZWVnPpzJs5OjtnoyTEbPTlmZzeScK6FIY8WLQHqTy2klHo/8BCwRmsdHJvqCSH+//buP7aq8o7j+PsL1F5ZJ2lX5YdsUDe0W7mtCC7oJqDdKEUrM1EAAAukSURBVCxKHem0ioaRhQ2ZohhNLQojKItuzk2jwXUbAbQbdLBOEpFtpIVOg46yNNYKdqaiVqSUtkFJLEh59se91FJ6L/fCLee0/bz+6b3nnnP6Pd886bfPc+55HhEZeGK557wLGGdmGWZ2AVAIbO66g5lNAH4PzHLOHUx8mCIiIgPHGYuzc+44cDfwD2APUOacqzOzFWY2K7zbr4EU4K9mVmNmmyOcTkRERM4gpuecnXNbgC3dti3r8vp7CY5LRERkwNL0nT24+eabmThxIllZWZSUlACwdetWrrrqKnJycsjNzQVC3zacN28ewWCQ7OxsNm3a5GXYIiLST/h3+s5XHoIDtYk954ggzDzzsiCrV68mLS2Nzz77jKuvvpr8/Hzmz59PVVUVGRkZtLa2AvDoo48ybNgwamtDcba1tSU2XhERGZD8W5w99Mwzz1BeXg7Ahx9+SElJCVOmTCEjIwOAtLQ0ALZt28b69es7j0tNTT3/wYqISL/j3+IcQw+3N2zfvp1t27axc+dOhg4dyrRp08jJyeGdd945bV/nHGZxrqAtIiJyBrrn3M3hw4dJTU1l6NCh7N27l9dff52jR4+yY8cO3nvvPYDOYe3p06fz7LPPdh6rYW0REUkEFeduZsyYwfHjx8nOzmbp0qVMnjyZiy++mJKSEmbPnk1OTg633norAI888ghtbW2MHz+enJwcKisrPY5eRET6A/8Oa3skOTmZV17pcWpwZs6cecr7lJQU1q5dez7CEhGRAUQ9ZxEREZ9RcRYREfEZFWcRERGfUXEWERHxGRVnERERn1FxFhER8RkVZxEREZ9RcT4HKSkpET/bt28f48ePP4/RiIhIf6HiLCIi4jO+nSHsif88wd7WvQk9Z2ZaJkXfLor4eVFREWPGjGHhwoUALF++HDOjqqqKtrY2Pv/8cx577DHy8/Pj+r3t7e3cddddVFdXM2TIEJ566imuv/566urqmDdvHseOHePEiRNs2rSJUaNGccstt9DY2EhHRwdLly7tnC5UREQGBt8WZy8UFhZy3333dRbnsrIytm7dyuLFi7nooos4dOgQkydPZtasWXGtRvXcc88BUFtby969e5k+fTr19fU8//zz3HvvvcyZM4djx47R0dHBli1bGDVqFC+//DIQWohDREQGFt8W52g93N4yYcIEDh48yP79+2lubiY1NZWRI0eyePFiqqqqGDRoEB999BFNTU2MGDEi5vO++uqr3HPPPQBkZmYyZswY6uvrueaaa1i5ciWNjY3Mnj2bcePGEQwGeeCBBygqKuLGG2/kuuuu663LFRERn9I9524KCgrYuHEjGzZsoLCwkNLSUpqbm9m9ezc1NTUMHz6c9vb2uM7pnOtx++23387mzZu58MILycvLo6Kigssvv5zdu3cTDAYpLi5mxYoVibgsERHpQ3zbc/ZKYWEh8+fP59ChQ+zYsYOysjIuueQSkpKSqKys5P3334/7nFOmTKG0tJQbbriB+vp6PvjgA6644goaGhq47LLLWLRoEQ0NDbz55ptkZmaSlpbGHXfcQUpKCmvWrEn8RYqIiK+pOHeTlZXFp59+yqWXXsrIkSOZM2cON910E5MmTeLKK68kMzMz7nMuXLiQBQsWEAwGGTJkCGvWrCE5OZkNGzbw4osvkpSUxIgRI1i2bBm7du3iwQcfZNCgQSQlJbFq1apeuEoREfEzFece1NbWdr5OT09n586dPe535MiRiOcYO3Ysb731FgCBQKDHHnBxcTHFxcWnbMvLyyMvL+8sohYRkf5C95xFRER8Rj3nc1RbW8udd955yrbk5GTeeOMNjyISEZG+TsX5HAWDQWpqarwOQ0RE+hENa4uIiPiMirOIiIjPqDiLiIj4jIqziIiIz6g4n4No6zmLiIicLRVnERERn/Hto1QHfvlLju5J7HrOyd/MZMSSJRE/T+R6zkeOHCE/P7/H49atW8eTTz6JmZGdnc0LL7xAU1MTCxYsoKGhAYBVq1Zx7bXXJuCqRUSkr/FtcfZCItdzDgQClJeXn3bc22+/zcqVK3nttddIT0+ntbUVgEWLFjF16lTKy8vp6OiIOjWoiIj0b74tztF6uL0lkes5O+dYsmTJacdVVFRQUFBAeno6AGlpaQBUVFSwbt06AAYPHsywYcN692JFRMS3fFucvXJyPecDBw6ctp5zUlISY8eOjWk950jHOefO2OsWEZGBTV8I66awsJD169ezceNGCgoKOHz48Fmt5xzpuNzcXMrKymhpaQHoHNbOzc3tXB6yo6ODTz75pBeuTkRE+gIV5256Ws+5urqaSZMmUVpaGvN6zpGOy8rK4uGHH2bq1Knk5ORw//33A/D0009TWVlJMBhk4sSJ1NXV9do1ioiIv2lYuweJWM852nFz585l7ty5p2wbPnw4L7300llEKyIi/Y16ziIiIj6jnvM50nrOIiKSaCrO50jrOYuISKL5bljbOed1CL6nHImI9G++Ks6BQICWlhYVnyicc7S0tBAIBLwORUREeomvhrVHjx5NY2Mjzc3NXofiS+3t7QQCAQKBAKNHj/Y6HBER6SUxFWczmwE8DQwG/uice7zb58nAOmAi0ALc6pzbF28wSUlJZGRkxHvYgLF9+3YmTJjgdRgiItLLzjisbWaDgeeAmcC3gNvM7FvddvsJ0Oac+wbwW+CJRAcqIiIyUMRyz/nbwLvOuQbn3DFgPdB9zcR8YG349UYg1zSBtIiIyFmJpThfCnzY5X1jeFuP+zjnjgOHga8kIkAREZGBJpZ7zj31gLt/nTqWfTCznwI/Db89amZvxfD75QvpwCGvg+hjlLP4KWfxU87iNxBzNibWHWMpzo3AV7u8Hw3sj7BPo5kNAYYBrd1P5JwrAUoAzKzaOTcp1kBFOTsbyln8lLP4KWfxU86ii2VYexcwzswyzOwCoBDY3G2fzcDJlRwKgAqnh5VFRETOyhl7zs6542Z2N/APQo9SrXbO1ZnZCqDaObcZ+BPwgpm9S6jHXNibQYuIiPRnMT3n7JzbAmzptm1Zl9ftwI/i/N0lce4vytnZUM7ip5zFTzmLn3IWhWn0WURExF98Nbe2iIiIeFSczWyGmb1jZu+a2UNexNDXmNk+M6s1sxozq/Y6Hj8ys9VmdrDrI3pmlmZm/zKz/4V/pnoZo99EyNlyM/so3NZqzOwHXsboJ2b2VTOrNLM9ZlZnZveGt6udRRAlZ2pnUZz3Ye3wdKD1wPcJPYK1C7jNOff2eQ2kjzGzfcAk59xAey4wZmY2BTgCrHPOjQ9v+xXQ6px7PPyPYKpzrsjLOP0kQs6WA0ecc096GZsfmdlIYKRz7r9m9mVgN3Az8GPUznoUJWe3oHYWkRc951imAxWJm3OuitOfr+86texaQn8UJCxCziQC59zHzrn/hl9/CuwhNEOi2lkEUXImUXhRnGOZDlRO54B/mtnu8ExrEpvhzrmPIfRHArjE43j6irvN7M3wsLeGaHtgZmOBCcAbqJ3FpFvOQO0sIi+Kc0xTfcppvuOcu4rQ6mA/Dw9HivSGVcDXgSuBj4HfeBuO/5hZCrAJuM8594nX8fQFPeRM7SwKL4pzLNOBSjfOuf3hnweBckK3B+TMmsL3vE7e+zrocTy+55xrcs51OOdOAH9Abe0UZpZEqMiUOuf+Ft6sdhZFTzlTO4vOi+Icy3Sg0oWZfSn8RQrM7EvAdECLhsSm69Syc4GXPIylTzhZZMJ+iNpap/BSuH8C9jjnnurykdpZBJFypnYWnSeTkIS/Mv87vpgOdOV5D6IPMbPLCPWWITSr25+Vs9OZ2V+AaYRWu2kCfgH8HSgDvgZ8APzIOacvQIVFyNk0QkONDtgH/Ozk/dSBzsy+C/wbqAVOhDcvIXQPVe2sB1FydhtqZxFphjARERGf0QxhIiIiPqPiLCIi4jMqziIiIj6j4iwiIuIzKs4iIiI+o+IsIiLiMyrOIiIiPqPiLCIi4jP/By/tqKNjNfdfAAAAAElFTkSuQmCC\n"
                    },
                    "metadata": {
                        "needs_background": "light"
                    }
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "model.evaluate(X_test, y_test)",
            "execution_count": 27,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "10000/10000 [==============================] - 2s 218us/sample - loss: 2.3274 - acc: 0.8534\n",
                    "name": "stdout"
                },
                {
                    "output_type": "execute_result",
                    "execution_count": 27,
                    "data": {
                        "text/plain": "[2.3273586376190156, 0.8534]"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Using the model to make predictions"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "X_new = X_test[:3]\ny_proba = model.predict(X_new)\ny_proba.round(2)",
            "execution_count": 28,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 28,
                    "data": {
                        "text/plain": "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "y_pred = model.predict_classes(X_new)\ny_pred",
            "execution_count": 29,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 29,
                    "data": {
                        "text/plain": "array([9, 2, 1])"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "np.array(class_names)[y_pred]",
            "execution_count": 30,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 30,
                    "data": {
                        "text/plain": "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "y_new = y_test[:3]\ny_new",
            "execution_count": 31,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 31,
                    "data": {
                        "text/plain": "array([9, 2, 1], dtype=uint8)"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Building a Regression MLP Using the Sequential API"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler",
            "execution_count": 32,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "housing = fetch_california_housing()",
            "execution_count": 33,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)",
            "execution_count": 34,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)",
            "execution_count": 35,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "scaler = StandardScaler()",
            "execution_count": 36,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "X_train = scaler.fit_transform(X_train)\nX_valid = scaler.fit_transform(X_valid)\nX_test = scaler.fit_transform(X_test)",
            "execution_count": 37,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "model = keras.models.Sequential([\n    keras.layers.Dense(30, activation = \"relu\", input_shape = X_train.shape[1:]),\n    keras.layers.Dense(1)\n])",
            "execution_count": 38,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "model.compile(loss = \"mean_squared_error\", optimizer = \"sgd\")\nhistory = model.fit(X_train, y_train, epochs = 15, validation_data = (X_valid, y_valid))",
            "execution_count": 39,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "WARNING:tensorflow:From /opt/conda/envs/Python36/lib/python3.6/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nTrain on 11610 samples, validate on 3870 samples\nWARNING:tensorflow:From /opt/conda/envs/Python36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nEpoch 1/15\n11610/11610 [==============================] - 2s 195us/sample - loss: 0.7335 - val_loss: 0.6233\nEpoch 2/15\n11610/11610 [==============================] - 3s 288us/sample - loss: 0.4652 - val_loss: 0.5644\nEpoch 3/15\n11610/11610 [==============================] - 2s 202us/sample - loss: 0.4556 - val_loss: 0.5819\nEpoch 4/15\n11610/11610 [==============================] - 2s 172us/sample - loss: 0.4298 - val_loss: 0.5542\nEpoch 5/15\n11610/11610 [==============================] - 2s 182us/sample - loss: 0.4329 - val_loss: 0.5106\nEpoch 6/15\n11610/11610 [==============================] - 2s 171us/sample - loss: 0.3882 - val_loss: 0.5056\nEpoch 7/15\n11610/11610 [==============================] - 3s 271us/sample - loss: 0.3807 - val_loss: 0.5327\nEpoch 8/15\n11610/11610 [==============================] - 2s 169us/sample - loss: 0.3961 - val_loss: 0.4888\nEpoch 9/15\n11610/11610 [==============================] - 2s 173us/sample - loss: 0.3727 - val_loss: 0.4838\nEpoch 10/15\n11610/11610 [==============================] - 2s 167us/sample - loss: 0.3689 - val_loss: 0.4821\nEpoch 11/15\n11610/11610 [==============================] - 2s 166us/sample - loss: 0.3667 - val_loss: 0.4866\nEpoch 12/15\n11610/11610 [==============================] - 3s 275us/sample - loss: 0.3646 - val_loss: 0.4914\nEpoch 13/15\n11610/11610 [==============================] - 2s 162us/sample - loss: 0.3630 - val_loss: 0.4828\nEpoch 14/15\n11610/11610 [==============================] - 2s 162us/sample - loss: 0.3594 - val_loss: 0.5064\nEpoch 15/15\n11610/11610 [==============================] - 2s 167us/sample - loss: 0.3581 - val_loss: 0.5033\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "mse_test = model.evaluate(X_test, y_test)",
            "execution_count": 40,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "5160/5160 [==============================] - 0s 73us/sample - loss: 0.4439\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "X_new = X_test[:3]",
            "execution_count": 41,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "y_pred = model.predict(X_new)",
            "execution_count": 42,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Building complex models using Functional APIs"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "input_ = keras.layers.Input(shape=X_train.shape[1:])\nhidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\nhidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\nconcat = keras.layers.Concatenate()([input_, hidden2])\noutput = keras.layers.Dense(1)(concat)\nmodel = keras.Model(inputs=[input_], outputs=[output])",
            "execution_count": 43,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "input_A = keras.layers.Input(shape = [5], name = \"wide_input\")\ninput_B = keras.layers.Input(shape = [6], name = \"deep_input\")\nhidden1 = keras.layers.Dense(30, activation = \"relu\")(input_B)\nhidden2 = keras.layers.Dense(30, activation = \"relu\")(hidden1)\nconcat = keras.layers.concatenate([input_A, hidden2])\noutput = keras.layers.Dense(1, name = \"output\")(concat)\nmodel = keras.Model(inputs= [input_A, input_B], outputs = [output])",
            "execution_count": 44,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "model.compile(loss = \"mse\", optimizer = keras.optimizers.SGD(lr = 1e-3))",
            "execution_count": 45,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "X_train_A, X_train_B = X_train[:,:5], X_train[:, 2:]\nX_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\nX_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\nX_new_A, X_new_B = X_test_A[:3], X_test_B[:3]",
            "execution_count": 46,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "history = model.fit((X_train_A, X_train_B), y_train, epochs = 15, validation_data = ((X_valid_A, X_valid_B), y_valid))",
            "execution_count": 47,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Train on 11610 samples, validate on 3870 samples\nEpoch 1/15\n11610/11610 [==============================] - 6s 500us/sample - loss: 1.9270 - val_loss: 1.0585\nEpoch 2/15\n11610/11610 [==============================] - 3s 295us/sample - loss: 0.7865 - val_loss: 0.8074\nEpoch 3/15\n11610/11610 [==============================] - 3s 297us/sample - loss: 0.6643 - val_loss: 0.7405\nEpoch 4/15\n11610/11610 [==============================] - 5s 446us/sample - loss: 0.6093 - val_loss: 0.7018\nEpoch 5/15\n11610/11610 [==============================] - 4s 321us/sample - loss: 0.5716 - val_loss: 0.6695\nEpoch 6/15\n11610/11610 [==============================] - 5s 461us/sample - loss: 0.5438 - val_loss: 0.6510\nEpoch 7/15\n11610/11610 [==============================] - 3s 296us/sample - loss: 0.5210 - val_loss: 0.6299\nEpoch 8/15\n11610/11610 [==============================] - 3s 297us/sample - loss: 0.5025 - val_loss: 0.6175\nEpoch 9/15\n11610/11610 [==============================] - 5s 430us/sample - loss: 0.4879 - val_loss: 0.6043\nEpoch 10/15\n11610/11610 [==============================] - 3s 296us/sample - loss: 0.4756 - val_loss: 0.5926\nEpoch 11/15\n11610/11610 [==============================] - 3s 288us/sample - loss: 0.4650 - val_loss: 0.5836\nEpoch 12/15\n11610/11610 [==============================] - 5s 467us/sample - loss: 0.4564 - val_loss: 0.5771\nEpoch 13/15\n11610/11610 [==============================] - 3s 289us/sample - loss: 0.4490 - val_loss: 0.5691\nEpoch 14/15\n11610/11610 [==============================] - 3s 286us/sample - loss: 0.4426 - val_loss: 0.5622\nEpoch 15/15\n11610/11610 [==============================] - 5s 408us/sample - loss: 0.4378 - val_loss: 0.5564\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\ny_pred = model.predict((X_new_A, X_new_B))",
            "execution_count": 48,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "5160/5160 [==============================] - 1s 128us/sample - loss: 0.4883\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "input_A = keras.layers.Input(shape = [5], name = \"wide_input\")\ninput_B = keras.layers.Input(shape = [6], name = \"deep_input\")\nhidden1 = keras.layers.Dense(30, activation = \"relu\")(input_B)\nhidden2 = keras.layers.Dense(30, activation = \"relu\")(hidden1)\nconcat = keras.layers.concatenate([input_A, hidden2])\noutput = keras.layers.Dense(1, name = \"output\")(concat)\naux_output = keras.layers.Dense(1, name = \"aux_output\")(hidden2)\nmodel = keras.Model(inputs= [input_A, input_B], outputs = [output, aux_output])",
            "execution_count": 49,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "model.compile(loss = [\"mse\", \"mse\"], loss_weights = [0.9, 0.1], optimizer= \"sgd\")",
            "execution_count": 50,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs =15, validation_data = ([X_valid_A, X_valid_B], [y_valid, y_valid]))",
            "execution_count": 51,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Train on 11610 samples, validate on 3870 samples\nEpoch 1/15\n11610/11610 [==============================] - 5s 421us/sample - loss: 1.0116 - output_loss: 0.9116 - aux_output_loss: 1.9079 - val_loss: 0.6758 - val_output_loss: 0.6134 - val_aux_output_loss: 1.2357\nEpoch 2/15\n11610/11610 [==============================] - 6s 474us/sample - loss: 0.5222 - output_loss: 0.4690 - aux_output_loss: 1.0009 - val_loss: 0.5977 - val_output_loss: 0.5528 - val_aux_output_loss: 1.0015\nEpoch 3/15\n11610/11610 [==============================] - 5s 439us/sample - loss: 0.4728 - output_loss: 0.4324 - aux_output_loss: 0.8359 - val_loss: 0.5703 - val_output_loss: 0.5347 - val_aux_output_loss: 0.8917\nEpoch 4/15\n11610/11610 [==============================] - 6s 518us/sample - loss: 0.4434 - output_loss: 0.4109 - aux_output_loss: 0.7366 - val_loss: 0.5672 - val_output_loss: 0.5392 - val_aux_output_loss: 0.8185\nEpoch 5/15\n11610/11610 [==============================] - 4s 356us/sample - loss: 0.4333 - output_loss: 0.4069 - aux_output_loss: 0.6700 - val_loss: 0.5582 - val_output_loss: 0.5334 - val_aux_output_loss: 0.7815\nEpoch 6/15\n11610/11610 [==============================] - 8s 661us/sample - loss: 0.4216 - output_loss: 0.3989 - aux_output_loss: 0.6269 - val_loss: 0.5485 - val_output_loss: 0.5254 - val_aux_output_loss: 0.7554\nEpoch 7/15\n11610/11610 [==============================] - 6s 475us/sample - loss: 0.4132 - output_loss: 0.3926 - aux_output_loss: 0.5993 - val_loss: 0.5364 - val_output_loss: 0.5135 - val_aux_output_loss: 0.7407\nEpoch 8/15\n11610/11610 [==============================] - 6s 498us/sample - loss: 0.4042 - output_loss: 0.3850 - aux_output_loss: 0.5769 - val_loss: 0.5405 - val_output_loss: 0.5187 - val_aux_output_loss: 0.7363\nEpoch 9/15\n11610/11610 [==============================] - 5s 396us/sample - loss: 0.3981 - output_loss: 0.3800 - aux_output_loss: 0.5625 - val_loss: 0.5416 - val_output_loss: 0.5202 - val_aux_output_loss: 0.7333\nEpoch 10/15\n11610/11610 [==============================] - 8s 709us/sample - loss: 0.3915 - output_loss: 0.3738 - aux_output_loss: 0.5506 - val_loss: 0.5405 - val_output_loss: 0.5196 - val_aux_output_loss: 0.7287\nEpoch 11/15\n11610/11610 [==============================] - 4s 349us/sample - loss: 0.3864 - output_loss: 0.3693 - aux_output_loss: 0.5397 - val_loss: 0.5421 - val_output_loss: 0.5217 - val_aux_output_loss: 0.7268\nEpoch 12/15\n11610/11610 [==============================] - 6s 509us/sample - loss: 0.3830 - output_loss: 0.3666 - aux_output_loss: 0.5310 - val_loss: 0.5500 - val_output_loss: 0.5301 - val_aux_output_loss: 0.7279\nEpoch 13/15\n11610/11610 [==============================] - 5s 408us/sample - loss: 0.3812 - output_loss: 0.3654 - aux_output_loss: 0.5251 - val_loss: 0.5462 - val_output_loss: 0.5256 - val_aux_output_loss: 0.7317\nEpoch 14/15\n11610/11610 [==============================] - 8s 659us/sample - loss: 0.3763 - output_loss: 0.3607 - aux_output_loss: 0.5165 - val_loss: 0.5464 - val_output_loss: 0.5255 - val_aux_output_loss: 0.7337\nEpoch 15/15\n11610/11610 [==============================] - 4s 359us/sample - loss: 0.3759 - output_loss: 0.3611 - aux_output_loss: 0.5110 - val_loss: 0.5229 - val_output_loss: 0.5022 - val_aux_output_loss: 0.7085\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "total_loss, main_loss, aux_loss = model.evaluate([X_test_A, X_test_B], [y_test, y_test])",
            "execution_count": 52,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "5160/5160 [==============================] - 1s 274us/sample - loss: 0.4386 - output_loss: 0.4199 - aux_output_loss: 0.5994\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])",
            "execution_count": 53,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Using the Subclassing API to Build Dynamic Models"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "class WideAndDeepModel(keras.Model):\n    def __init__(self, units=30, activation=\"relu\", **kwargs):\n        super().__init__(**kwargs) # handles standard args (e.g., name)\n        self.hidden1 = keras.layers.Dense(units, activation=activation)\n        self.hidden2 = keras.layers.Dense(units, activation=activation)\n        self.main_output = keras.layers.Dense(1)\n        self.aux_output = keras.layers.Dense(1)\n    def call(self, inputs):\n        input_A, input_B = inputs\n        hidden1 = self.hidden1(input_B)\n        hidden2 = self.hidden2(hidden1)\n        concat = keras.layers.concatenate([input_A, hidden2])\n        main_output = self.main_output(concat)\n        aux_output = self.aux_output(hidden2)\n        return main_output, aux_output",
            "execution_count": 54,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Saving and Restoring Models"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "model.save(\"my_keras.h5\")",
            "execution_count": 55,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "model = keras.models.load_model(\"my_keras.h5\")",
            "execution_count": 56,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras.h5\")\nhistory = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs =15, validation_data = ([X_valid_A, X_valid_B], [y_valid, y_valid]), callbacks = [checkpoint_cb])",
            "execution_count": 57,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Train on 11610 samples, validate on 3870 samples\nEpoch 1/15\n11610/11610 [==============================] - 7s 579us/sample - loss: 0.3709 - output_loss: 0.3558 - aux_output_loss: 0.5064 - val_loss: 0.5340 - val_output_loss: 0.5145 - val_aux_output_loss: 0.7093\nEpoch 2/15\n11610/11610 [==============================] - 7s 632us/sample - loss: 0.3705 - output_loss: 0.3562 - aux_output_loss: 0.5001 - val_loss: 0.5329 - val_output_loss: 0.5140 - val_aux_output_loss: 0.7007\nEpoch 3/15\n11610/11610 [==============================] - 4s 360us/sample - loss: 0.3648 - output_loss: 0.3507 - aux_output_loss: 0.4933 - val_loss: 0.5425 - val_output_loss: 0.5238 - val_aux_output_loss: 0.7095\nEpoch 4/15\n11610/11610 [==============================] - 7s 566us/sample - loss: 0.3620 - output_loss: 0.3479 - aux_output_loss: 0.4888 - val_loss: 0.5263 - val_output_loss: 0.5069 - val_aux_output_loss: 0.7007\nEpoch 5/15\n11610/11610 [==============================] - 5s 389us/sample - loss: 0.3598 - output_loss: 0.3459 - aux_output_loss: 0.4843 - val_loss: 0.5366 - val_output_loss: 0.5187 - val_aux_output_loss: 0.6971\nEpoch 6/15\n11610/11610 [==============================] - 6s 501us/sample - loss: 0.3580 - output_loss: 0.3445 - aux_output_loss: 0.4791 - val_loss: 0.5520 - val_output_loss: 0.5329 - val_aux_output_loss: 0.7220\nEpoch 7/15\n11610/11610 [==============================] - 4s 361us/sample - loss: 0.3590 - output_loss: 0.3457 - aux_output_loss: 0.4778 - val_loss: 0.5302 - val_output_loss: 0.5107 - val_aux_output_loss: 0.7048\nEpoch 8/15\n11610/11610 [==============================] - 9s 752us/sample - loss: 0.3562 - output_loss: 0.3432 - aux_output_loss: 0.4741 - val_loss: 0.5198 - val_output_loss: 0.5015 - val_aux_output_loss: 0.6879\nEpoch 9/15\n11610/11610 [==============================] - 6s 474us/sample - loss: 0.3542 - output_loss: 0.3413 - aux_output_loss: 0.4701 - val_loss: 0.5168 - val_output_loss: 0.4983 - val_aux_output_loss: 0.6828\nEpoch 10/15\n11610/11610 [==============================] - 8s 690us/sample - loss: 0.3524 - output_loss: 0.3396 - aux_output_loss: 0.4671 - val_loss: 0.5227 - val_output_loss: 0.5037 - val_aux_output_loss: 0.6936\nEpoch 11/15\n11610/11610 [==============================] - 6s 551us/sample - loss: 0.3493 - output_loss: 0.3367 - aux_output_loss: 0.4626 - val_loss: 0.5263 - val_output_loss: 0.5087 - val_aux_output_loss: 0.6886\nEpoch 12/15\n11610/11610 [==============================] - 6s 482us/sample - loss: 0.3486 - output_loss: 0.3363 - aux_output_loss: 0.4597 - val_loss: 0.5258 - val_output_loss: 0.5079 - val_aux_output_loss: 0.6864\nEpoch 13/15\n11610/11610 [==============================] - 6s 492us/sample - loss: 0.3463 - output_loss: 0.3340 - aux_output_loss: 0.4561 - val_loss: 0.5339 - val_output_loss: 0.5163 - val_aux_output_loss: 0.6924\nEpoch 14/15\n11610/11610 [==============================] - 4s 356us/sample - loss: 0.3443 - output_loss: 0.3322 - aux_output_loss: 0.4539 - val_loss: 0.5249 - val_output_loss: 0.5076 - val_aux_output_loss: 0.6800\nEpoch 15/15\n11610/11610 [==============================] - 7s 608us/sample - loss: 0.3427 - output_loss: 0.3308 - aux_output_loss: 0.4498 - val_loss: 0.5350 - val_output_loss: 0.5178 - val_aux_output_loss: 0.6903\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras.h5\", save_best_only = True) ## by setting save_best_only to True, it saves the best possible model\nhistory = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs =15, validation_data = ([X_valid_A, X_valid_B], [y_valid, y_valid]), callbacks = [checkpoint_cb])",
            "execution_count": 58,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Train on 11610 samples, validate on 3870 samples\nEpoch 1/15\n11610/11610 [==============================] - 4s 355us/sample - loss: 0.3427 - output_loss: 0.3308 - aux_output_loss: 0.4486 - val_loss: 0.5252 - val_output_loss: 0.5080 - val_aux_output_loss: 0.6791\nEpoch 2/15\n11610/11610 [==============================] - 7s 589us/sample - loss: 0.3409 - output_loss: 0.3297 - aux_output_loss: 0.4448 - val_loss: 0.5266 - val_output_loss: 0.5092 - val_aux_output_loss: 0.6858\nEpoch 3/15\n11610/11610 [==============================] - 4s 363us/sample - loss: 0.3412 - output_loss: 0.3300 - aux_output_loss: 0.4433 - val_loss: 0.5269 - val_output_loss: 0.5101 - val_aux_output_loss: 0.6776\nEpoch 4/15\n11610/11610 [==============================] - 4s 361us/sample - loss: 0.3391 - output_loss: 0.3279 - aux_output_loss: 0.4399 - val_loss: 0.5251 - val_output_loss: 0.5091 - val_aux_output_loss: 0.6701\nEpoch 5/15\n11610/11610 [==============================] - 6s 540us/sample - loss: 0.3378 - output_loss: 0.3266 - aux_output_loss: 0.4382 - val_loss: 0.5318 - val_output_loss: 0.5151 - val_aux_output_loss: 0.6815\nEpoch 6/15\n11610/11610 [==============================] - 6s 498us/sample - loss: 0.3371 - output_loss: 0.3259 - aux_output_loss: 0.4363 - val_loss: 0.5083 - val_output_loss: 0.4899 - val_aux_output_loss: 0.6736\nEpoch 7/15\n11610/11610 [==============================] - 6s 499us/sample - loss: 0.3372 - output_loss: 0.3264 - aux_output_loss: 0.4351 - val_loss: 0.5110 - val_output_loss: 0.4935 - val_aux_output_loss: 0.6689\nEpoch 8/15\n11610/11610 [==============================] - 5s 396us/sample - loss: 0.3341 - output_loss: 0.3233 - aux_output_loss: 0.4309 - val_loss: 0.5127 - val_output_loss: 0.4948 - val_aux_output_loss: 0.6755\nEpoch 9/15\n11610/11610 [==============================] - 6s 504us/sample - loss: 0.3343 - output_loss: 0.3235 - aux_output_loss: 0.4309 - val_loss: 0.5008 - val_output_loss: 0.4842 - val_aux_output_loss: 0.6495\nEpoch 10/15\n11610/11610 [==============================] - 8s 687us/sample - loss: 0.3336 - output_loss: 0.3232 - aux_output_loss: 0.4286 - val_loss: 0.4947 - val_output_loss: 0.4766 - val_aux_output_loss: 0.6578\nEpoch 11/15\n11610/11610 [==============================] - 4s 363us/sample - loss: 0.3323 - output_loss: 0.3218 - aux_output_loss: 0.4257 - val_loss: 0.5287 - val_output_loss: 0.5130 - val_aux_output_loss: 0.6707\nEpoch 12/15\n11610/11610 [==============================] - 4s 358us/sample - loss: 0.3312 - output_loss: 0.3209 - aux_output_loss: 0.4242 - val_loss: 0.5143 - val_output_loss: 0.4968 - val_aux_output_loss: 0.6717\nEpoch 13/15\n11610/11610 [==============================] - 8s 658us/sample - loss: 0.3298 - output_loss: 0.3196 - aux_output_loss: 0.4217 - val_loss: 0.5158 - val_output_loss: 0.4986 - val_aux_output_loss: 0.6695\nEpoch 14/15\n11610/11610 [==============================] - 8s 692us/sample - loss: 0.3302 - output_loss: 0.3200 - aux_output_loss: 0.4212 - val_loss: 0.5128 - val_output_loss: 0.4962 - val_aux_output_loss: 0.6670\nEpoch 15/15\n11610/11610 [==============================] - 4s 372us/sample - loss: 0.3299 - output_loss: 0.3200 - aux_output_loss: 0.4180 - val_loss: 0.5544 - val_output_loss: 0.5376 - val_aux_output_loss: 0.7035\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### FineTuning Neural Networks Hyperparameters"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3,input_shape=[8]):\n    model = keras.models.Sequential()\n    model.add(keras.layers.InputLayer(input_shape=input_shape))\n    for layer in range(n_hidden):\n        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n    model.add(keras.layers.Dense(1))\n    optimizer = keras.optimizers.SGD(lr=learning_rate)\n    model.compile(loss=\"mse\", optimizer=optimizer)\n    return model",
            "execution_count": 59,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)",
            "execution_count": 60,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "keras_reg.fit(X_train, y_train, epochs = 10, validation_data = (X_valid, y_valid))",
            "execution_count": 61,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Train on 11610 samples, validate on 3870 samples\nEpoch 1/10\n11610/11610 [==============================] - 3s 222us/sample - loss: 1.2055 - val_loss: 0.7517\nEpoch 2/10\n11610/11610 [==============================] - 4s 315us/sample - loss: 0.6035 - val_loss: 0.6566\nEpoch 3/10\n11610/11610 [==============================] - 2s 171us/sample - loss: 0.5427 - val_loss: 0.6068\nEpoch 4/10\n11610/11610 [==============================] - 2s 170us/sample - loss: 0.4966 - val_loss: 0.5730\nEpoch 5/10\n11610/11610 [==============================] - 2s 173us/sample - loss: 0.4626 - val_loss: 0.5566\nEpoch 6/10\n11610/11610 [==============================] - 2s 168us/sample - loss: 0.4491 - val_loss: 0.5349\nEpoch 7/10\n11610/11610 [==============================] - 4s 376us/sample - loss: 0.4342 - val_loss: 0.5273\nEpoch 8/10\n11610/11610 [==============================] - 2s 173us/sample - loss: 0.4226 - val_loss: 0.5117\nEpoch 9/10\n11610/11610 [==============================] - 3s 227us/sample - loss: 0.4148 - val_loss: 0.5131\nEpoch 10/10\n11610/11610 [==============================] - 2s 176us/sample - loss: 0.4064 - val_loss: 0.5031\n",
                    "name": "stdout"
                },
                {
                    "output_type": "execute_result",
                    "execution_count": 61,
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f71b7f4ddd8>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "mse_test = keras_reg.score(X_test, y_test)",
            "execution_count": 62,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "5160/5160 [==============================] - 0s 76us/sample - loss: 0.4577\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "y_pred = keras_reg.predict(X_new)",
            "execution_count": 63,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from scipy.stats import reciprocal\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_distribs = {\"n_hidden\": [0,1,2,3], \"n_neurons\": np.arange(1,100), \"learning_rate\": reciprocal(3e-4, 3e-2)}",
            "execution_count": 64,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "rnd_cv = RandomizedSearchCV(keras_reg, param_distribs, cv = 5)",
            "execution_count": 65,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "rnd_cv.fit(X_train, y_train, epochs = 10, validation_data = (X_valid, y_valid), callbacks = [keras.callbacks.EarlyStopping(patience = 10)])",
            "execution_count": 66,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Train on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 1s 86us/sample - loss: 1.5585 - val_loss: 0.8167\nEpoch 2/10\n9288/9288 [==============================] - 0s 30us/sample - loss: 0.8687 - val_loss: 0.6689\nEpoch 3/10\n9288/9288 [==============================] - 0s 30us/sample - loss: 0.5774 - val_loss: 0.6043\nEpoch 4/10\n9288/9288 [==============================] - 0s 30us/sample - loss: 0.5342 - val_loss: 0.5942\nEpoch 5/10\n9288/9288 [==============================] - 0s 30us/sample - loss: 0.5838 - val_loss: 0.8831\nEpoch 6/10\n9288/9288 [==============================] - 0s 30us/sample - loss: 0.5879 - val_loss: 0.6743\nEpoch 7/10\n9288/9288 [==============================] - 0s 33us/sample - loss: 0.8708 - val_loss: 0.6384\nEpoch 8/10\n9288/9288 [==============================] - 0s 31us/sample - loss: 0.5280 - val_loss: 0.6193\nEpoch 9/10\n9288/9288 [==============================] - 0s 30us/sample - loss: 0.7546 - val_loss: 0.6035\nEpoch 10/10\n9288/9288 [==============================] - 0s 31us/sample - loss: 0.5448 - val_loss: 0.6620\n2322/2322 [==============================] - 0s 14us/sample - loss: 0.5729\n9288/9288 [==============================] - 0s 14us/sample - loss: 0.6637\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 1s 88us/sample - loss: 1.4411 - val_loss: 0.7822\nEpoch 2/10\n9288/9288 [==============================] - 0s 33us/sample - loss: 0.5887 - val_loss: 0.7424\nEpoch 3/10\n9288/9288 [==============================] - 0s 32us/sample - loss: 0.5415 - val_loss: 0.7338\nEpoch 4/10\n9288/9288 [==============================] - 0s 30us/sample - loss: 0.5195 - val_loss: 0.6738\nEpoch 5/10\n9288/9288 [==============================] - 0s 33us/sample - loss: 0.5076 - val_loss: 0.6535\nEpoch 6/10\n9288/9288 [==============================] - 0s 32us/sample - loss: 0.5001 - val_loss: 0.6429\nEpoch 7/10\n9288/9288 [==============================] - 0s 30us/sample - loss: 0.4962 - val_loss: 0.6437\nEpoch 8/10\n9288/9288 [==============================] - 0s 32us/sample - loss: 0.4934 - val_loss: 0.6373\nEpoch 9/10\n9288/9288 [==============================] - 0s 32us/sample - loss: 0.4924 - val_loss: 0.6145\nEpoch 10/10\n9288/9288 [==============================] - 0s 30us/sample - loss: 0.4918 - val_loss: 0.6217\n2322/2322 [==============================] - 0s 14us/sample - loss: 0.8863\n9288/9288 [==============================] - 0s 14us/sample - loss: 0.4881\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 1s 90us/sample - loss: 1.9447 - val_loss: 0.8825\nEpoch 2/10\n9288/9288 [==============================] - 0s 30us/sample - loss: 0.6546 - val_loss: 0.8411\nEpoch 3/10\n9288/9288 [==============================] - 0s 32us/sample - loss: 1.1059 - val_loss: 0.7367\nEpoch 4/10\n9288/9288 [==============================] - 0s 33us/sample - loss: 0.6092 - val_loss: 0.6831\nEpoch 5/10\n9288/9288 [==============================] - 0s 31us/sample - loss: 0.6526 - val_loss: 1.0197\nEpoch 6/10\n9288/9288 [==============================] - 0s 31us/sample - loss: 0.5960 - val_loss: 0.6294\nEpoch 7/10\n9288/9288 [==============================] - 0s 32us/sample - loss: 0.6259 - val_loss: 0.6592\nEpoch 8/10\n9288/9288 [==============================] - 0s 32us/sample - loss: 0.5350 - val_loss: 0.5805\nEpoch 9/10\n9288/9288 [==============================] - 0s 31us/sample - loss: 0.6275 - val_loss: 0.5720\nEpoch 10/10\n9288/9288 [==============================] - 0s 32us/sample - loss: 0.5422 - val_loss: 0.5547\n2322/2322 [==============================] - 0s 15us/sample - loss: 0.5459\n9288/9288 [==============================] - 0s 15us/sample - loss: 0.5113\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 1s 96us/sample - loss: 2.0482 - val_loss: 1.9944\nEpoch 2/10\n9288/9288 [==============================] - 0s 31us/sample - loss: 1.6725 - val_loss: 2.8353\nEpoch 3/10\n9288/9288 [==============================] - 0s 31us/sample - loss: 3.2571 - val_loss: 5.4094\nEpoch 4/10\n9288/9288 [==============================] - 0s 31us/sample - loss: 3.8886 - val_loss: 8.5862\nEpoch 5/10\n9288/9288 [==============================] - 0s 31us/sample - loss: 13.6007 - val_loss: 15.7181\nEpoch 6/10\n9288/9288 [==============================] - 0s 31us/sample - loss: 7.6186 - val_loss: 25.6800\nEpoch 7/10\n9288/9288 [==============================] - 0s 33us/sample - loss: 44.2145 - val_loss: 47.4400\nEpoch 8/10\n9288/9288 [==============================] - 0s 31us/sample - loss: 32.9871 - val_loss: 86.8872\nEpoch 9/10\n9288/9288 [==============================] - 0s 31us/sample - loss: 76.8220 - val_loss: 170.3313\nEpoch 10/10\n9288/9288 [==============================] - 0s 31us/sample - loss: 157.0905 - val_loss: 323.8036\n2322/2322 [==============================] - 0s 14us/sample - loss: 140.9886\n9288/9288 [==============================] - 0s 14us/sample - loss: 364.5547\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 1s 94us/sample - loss: 1.4708 - val_loss: 0.8135\nEpoch 2/10\n9288/9288 [==============================] - 0s 31us/sample - loss: 0.9552 - val_loss: 0.8269\nEpoch 3/10\n9288/9288 [==============================] - 0s 32us/sample - loss: 0.7000 - val_loss: 0.8090\nEpoch 4/10\n9288/9288 [==============================] - 0s 32us/sample - loss: 0.6989 - val_loss: 1.6622\nEpoch 5/10\n9288/9288 [==============================] - 0s 32us/sample - loss: 0.8699 - val_loss: 0.9398\nEpoch 6/10\n9288/9288 [==============================] - 0s 32us/sample - loss: 1.1651 - val_loss: 1.2152\nEpoch 7/10\n9288/9288 [==============================] - 0s 33us/sample - loss: 0.7768 - val_loss: 0.8847\nEpoch 8/10\n9288/9288 [==============================] - 0s 35us/sample - loss: 0.9417 - val_loss: 0.8415\nEpoch 9/10\n9288/9288 [==============================] - 0s 33us/sample - loss: 0.5687 - val_loss: 0.5671\nEpoch 10/10\n9288/9288 [==============================] - 0s 31us/sample - loss: 0.5800 - val_loss: 0.5908\n2322/2322 [==============================] - 0s 15us/sample - loss: 0.5276\n9288/9288 [==============================] - 0s 15us/sample - loss: 0.5646\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 1s 100us/sample - loss: 1.8442 - val_loss: 0.9603\nEpoch 2/10\n9288/9288 [==============================] - 0s 33us/sample - loss: 0.6803 - val_loss: 0.6951\nEpoch 3/10\n9288/9288 [==============================] - 0s 33us/sample - loss: 0.6745 - val_loss: 0.7360\nEpoch 4/10\n9288/9288 [==============================] - 0s 33us/sample - loss: 0.5744 - val_loss: 0.5602\nEpoch 5/10\n9288/9288 [==============================] - 0s 33us/sample - loss: 0.5479 - val_loss: 0.6874\nEpoch 6/10\n9288/9288 [==============================] - 0s 33us/sample - loss: 0.5484 - val_loss: 0.5968\nEpoch 7/10\n9288/9288 [==============================] - 0s 33us/sample - loss: 0.6202 - val_loss: 0.5589\nEpoch 8/10\n9288/9288 [==============================] - 0s 33us/sample - loss: 0.5155 - val_loss: 0.5614\nEpoch 9/10\n9288/9288 [==============================] - 0s 35us/sample - loss: 0.5529 - val_loss: 0.6879\nEpoch 10/10\n9288/9288 [==============================] - 0s 31us/sample - loss: 0.5549 - val_loss: 0.5607\n2322/2322 [==============================] - 0s 15us/sample - loss: 0.4918\n9288/9288 [==============================] - 0s 14us/sample - loss: 0.5247\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 1s 103us/sample - loss: 1.5200 - val_loss: 0.6790\nEpoch 2/10\n9288/9288 [==============================] - 0s 33us/sample - loss: 0.5567 - val_loss: 0.6214\nEpoch 3/10\n9288/9288 [==============================] - 0s 32us/sample - loss: 0.5289 - val_loss: 0.6371\nEpoch 4/10\n9288/9288 [==============================] - 0s 32us/sample - loss: 0.5187 - val_loss: 0.6463\nEpoch 5/10\n9288/9288 [==============================] - 0s 32us/sample - loss: 0.5121 - val_loss: 0.6452\nEpoch 6/10\n9288/9288 [==============================] - 0s 33us/sample - loss: 0.5072 - val_loss: 0.6485\nEpoch 7/10\n",
                    "name": "stdout"
                },
                {
                    "output_type": "stream",
                    "text": "9288/9288 [==============================] - 0s 33us/sample - loss: 0.5030 - val_loss: 0.6526\nEpoch 8/10\n9288/9288 [==============================] - 0s 32us/sample - loss: 0.5006 - val_loss: 0.6406\nEpoch 9/10\n9288/9288 [==============================] - 0s 34us/sample - loss: 0.4982 - val_loss: 0.6508\nEpoch 10/10\n9288/9288 [==============================] - 0s 33us/sample - loss: 0.4964 - val_loss: 0.6488\n2322/2322 [==============================] - 0s 15us/sample - loss: 0.9754\n9288/9288 [==============================] - 0s 15us/sample - loss: 0.4926\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 1s 108us/sample - loss: 2.1220 - val_loss: 1.1634\nEpoch 2/10\n9288/9288 [==============================] - 0s 33us/sample - loss: 0.7087 - val_loss: 0.7312\nEpoch 3/10\n9288/9288 [==============================] - 0s 32us/sample - loss: 0.6696 - val_loss: 0.8719\nEpoch 4/10\n9288/9288 [==============================] - 0s 32us/sample - loss: 0.6893 - val_loss: 0.7070\nEpoch 5/10\n9288/9288 [==============================] - 0s 34us/sample - loss: 0.7187 - val_loss: 0.6431\nEpoch 6/10\n9288/9288 [==============================] - 0s 33us/sample - loss: 0.5388 - val_loss: 0.5638\nEpoch 7/10\n9288/9288 [==============================] - 0s 34us/sample - loss: 0.5314 - val_loss: 0.6366\nEpoch 8/10\n9288/9288 [==============================] - 0s 35us/sample - loss: 0.5337 - val_loss: 0.6167\nEpoch 9/10\n9288/9288 [==============================] - 0s 33us/sample - loss: 0.6103 - val_loss: 0.5619\nEpoch 10/10\n9288/9288 [==============================] - 0s 34us/sample - loss: 0.5276 - val_loss: 0.6191\n2322/2322 [==============================] - 0s 15us/sample - loss: 0.5856\n9288/9288 [==============================] - 0s 16us/sample - loss: 0.6006\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 1s 108us/sample - loss: 1.7245 - val_loss: 1.2382\nEpoch 2/10\n9288/9288 [==============================] - 0s 33us/sample - loss: 0.8225 - val_loss: 0.8809\nEpoch 3/10\n9288/9288 [==============================] - 0s 35us/sample - loss: 0.7914 - val_loss: 1.3991\nEpoch 4/10\n9288/9288 [==============================] - 0s 36us/sample - loss: 0.9092 - val_loss: 1.0492\nEpoch 5/10\n9288/9288 [==============================] - 0s 36us/sample - loss: 0.9715 - val_loss: 1.4599\nEpoch 6/10\n9288/9288 [==============================] - 0s 34us/sample - loss: 0.8623 - val_loss: 0.9645\nEpoch 7/10\n9288/9288 [==============================] - 0s 35us/sample - loss: 1.1006 - val_loss: 1.0702\nEpoch 8/10\n9288/9288 [==============================] - 0s 35us/sample - loss: 0.9202 - val_loss: 1.0887\nEpoch 9/10\n9288/9288 [==============================] - 0s 35us/sample - loss: 1.1693 - val_loss: 1.3031\nEpoch 10/10\n9288/9288 [==============================] - 0s 35us/sample - loss: 0.8197 - val_loss: 0.9450\n2322/2322 [==============================] - 0s 16us/sample - loss: 0.6109\n9288/9288 [==============================] - 0s 16us/sample - loss: 0.9303\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 1s 109us/sample - loss: 2.3779 - val_loss: 0.8646\nEpoch 2/10\n9288/9288 [==============================] - 0s 33us/sample - loss: 0.6452 - val_loss: 0.7727\nEpoch 3/10\n9288/9288 [==============================] - 0s 32us/sample - loss: 0.6787 - val_loss: 0.9990\nEpoch 4/10\n9288/9288 [==============================] - 0s 34us/sample - loss: 0.6646 - val_loss: 0.7086\nEpoch 5/10\n9288/9288 [==============================] - 0s 33us/sample - loss: 0.6963 - val_loss: 0.8853\nEpoch 6/10\n9288/9288 [==============================] - 0s 33us/sample - loss: 0.6147 - val_loss: 0.5912\nEpoch 7/10\n9288/9288 [==============================] - 0s 33us/sample - loss: 0.5797 - val_loss: 0.7201\nEpoch 8/10\n9288/9288 [==============================] - 0s 34us/sample - loss: 0.5682 - val_loss: 0.5601\nEpoch 9/10\n9288/9288 [==============================] - 0s 33us/sample - loss: 0.5354 - val_loss: 0.5596\nEpoch 10/10\n9288/9288 [==============================] - 0s 33us/sample - loss: 0.5620 - val_loss: 0.5868\n2322/2322 [==============================] - 0s 16us/sample - loss: 0.5329\n9288/9288 [==============================] - 0s 16us/sample - loss: 0.5637\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 7s 800us/sample - loss: 0.9256 - val_loss: 0.6444\nEpoch 2/10\n9288/9288 [==============================] - 4s 466us/sample - loss: 0.4633 - val_loss: 0.5504\nEpoch 3/10\n9288/9288 [==============================] - 8s 841us/sample - loss: 0.4096 - val_loss: 0.5622\nEpoch 4/10\n9288/9288 [==============================] - 4s 438us/sample - loss: 0.3905 - val_loss: 0.5020\nEpoch 5/10\n9288/9288 [==============================] - 7s 726us/sample - loss: 0.3685 - val_loss: 0.4991\nEpoch 6/10\n9288/9288 [==============================] - 4s 431us/sample - loss: 0.3545 - val_loss: 0.4922\nEpoch 7/10\n9288/9288 [==============================] - 9s 937us/sample - loss: 0.3487 - val_loss: 0.4945\nEpoch 8/10\n9288/9288 [==============================] - 5s 555us/sample - loss: 0.3420 - val_loss: 0.5323\nEpoch 9/10\n9288/9288 [==============================] - 5s 592us/sample - loss: 0.3394 - val_loss: 0.4642\nEpoch 10/10\n9288/9288 [==============================] - 9s 930us/sample - loss: 0.3334 - val_loss: 0.4530\n2322/2322 [==============================] - 1s 634us/sample - loss: 0.3455\n9288/9288 [==============================] - 3s 307us/sample - loss: 0.3287\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 7s 710us/sample - loss: 0.8949 - val_loss: 0.6669\nEpoch 2/10\n9288/9288 [==============================] - 4s 466us/sample - loss: 0.4827 - val_loss: 0.5442\nEpoch 3/10\n9288/9288 [==============================] - 4s 439us/sample - loss: 0.4166 - val_loss: 0.5187\nEpoch 4/10\n9288/9288 [==============================] - 7s 715us/sample - loss: 0.3884 - val_loss: 0.4913\nEpoch 5/10\n9288/9288 [==============================] - 5s 588us/sample - loss: 0.3720 - val_loss: 0.4918\nEpoch 6/10\n9288/9288 [==============================] - 7s 794us/sample - loss: 0.3622 - val_loss: 0.4779\nEpoch 7/10\n9288/9288 [==============================] - 4s 441us/sample - loss: 0.3530 - val_loss: 0.4860\nEpoch 8/10\n9288/9288 [==============================] - 6s 655us/sample - loss: 0.3477 - val_loss: 0.4772\nEpoch 9/10\n9288/9288 [==============================] - 5s 493us/sample - loss: 0.3421 - val_loss: 0.4790\nEpoch 10/10\n9288/9288 [==============================] - 7s 726us/sample - loss: 0.3366 - val_loss: 0.4536\n2322/2322 [==============================] - 0s 195us/sample - loss: 0.3642\n9288/9288 [==============================] - 2s 179us/sample - loss: 0.3519\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 7s 725us/sample - loss: 0.7837 - val_loss: 0.5946\nEpoch 2/10\n9288/9288 [==============================] - 4s 439us/sample - loss: 0.4878 - val_loss: 0.5190\nEpoch 3/10\n9288/9288 [==============================] - 5s 550us/sample - loss: 0.4054 - val_loss: 0.4945\nEpoch 4/10\n9288/9288 [==============================] - 5s 516us/sample - loss: 0.3843 - val_loss: 0.4805\nEpoch 5/10\n9288/9288 [==============================] - 4s 443us/sample - loss: 0.3696 - val_loss: 0.4670\nEpoch 6/10\n9288/9288 [==============================] - 6s 592us/sample - loss: 0.3608 - val_loss: 0.4708\nEpoch 7/10\n9288/9288 [==============================] - 5s 548us/sample - loss: 0.3517 - val_loss: 0.4624\nEpoch 8/10\n9288/9288 [==============================] - 8s 861us/sample - loss: 0.3538 - val_loss: 0.4589\nEpoch 9/10\n9288/9288 [==============================] - 4s 441us/sample - loss: 0.3388 - val_loss: 0.4686\nEpoch 10/10\n9288/9288 [==============================] - 8s 833us/sample - loss: 0.3332 - val_loss: 0.4841\n2322/2322 [==============================] - 0s 195us/sample - loss: 0.3529\n9288/9288 [==============================] - 2s 181us/sample - loss: 0.3391\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n",
                    "name": "stdout"
                },
                {
                    "output_type": "stream",
                    "text": "9288/9288 [==============================] - 9s 1ms/sample - loss: 1.3771 - val_loss: 0.6514\nEpoch 2/10\n9288/9288 [==============================] - 5s 526us/sample - loss: 0.4795 - val_loss: 0.5567\nEpoch 3/10\n9288/9288 [==============================] - 6s 645us/sample - loss: 0.3961 - val_loss: 0.5226\nEpoch 4/10\n9288/9288 [==============================] - 4s 444us/sample - loss: 0.3693 - val_loss: 0.5169\nEpoch 5/10\n9288/9288 [==============================] - 7s 747us/sample - loss: 0.3567 - val_loss: 0.4888\nEpoch 6/10\n9288/9288 [==============================] - 6s 676us/sample - loss: 0.3487 - val_loss: 0.4778\nEpoch 7/10\n9288/9288 [==============================] - 4s 465us/sample - loss: 0.3430 - val_loss: 0.4848\nEpoch 8/10\n9288/9288 [==============================] - 5s 536us/sample - loss: 0.3374 - val_loss: 0.4695\nEpoch 9/10\n9288/9288 [==============================] - 6s 654us/sample - loss: 0.3348 - val_loss: 0.4731\nEpoch 10/10\n9288/9288 [==============================] - 4s 428us/sample - loss: 0.3291 - val_loss: 0.4797\n2322/2322 [==============================] - 1s 476us/sample - loss: 0.3285\n9288/9288 [==============================] - 3s 341us/sample - loss: 0.3258\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 5s 566us/sample - loss: 0.8518 - val_loss: 0.6396\nEpoch 2/10\n9288/9288 [==============================] - 6s 593us/sample - loss: 0.4620 - val_loss: 0.5908\nEpoch 3/10\n9288/9288 [==============================] - 5s 551us/sample - loss: 0.4981 - val_loss: 0.5380\nEpoch 4/10\n9288/9288 [==============================] - 6s 680us/sample - loss: 0.3969 - val_loss: 0.4827\nEpoch 5/10\n9288/9288 [==============================] - 6s 655us/sample - loss: 0.3773 - val_loss: 0.4852\nEpoch 6/10\n9288/9288 [==============================] - 8s 824us/sample - loss: 0.3681 - val_loss: 0.4734\nEpoch 7/10\n9288/9288 [==============================] - 6s 684us/sample - loss: 0.3585 - val_loss: 0.4650\nEpoch 8/10\n9288/9288 [==============================] - 7s 734us/sample - loss: 0.3519 - val_loss: 0.4853\nEpoch 9/10\n9288/9288 [==============================] - 5s 501us/sample - loss: 0.3477 - val_loss: 0.4894\nEpoch 10/10\n9288/9288 [==============================] - 11s 1ms/sample - loss: 0.3449 - val_loss: 0.4718\n2322/2322 [==============================] - 0s 183us/sample - loss: 0.3255\n9288/9288 [==============================] - 3s 372us/sample - loss: 0.3376\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 5s 561us/sample - loss: 1.1120 - val_loss: 0.7431\nEpoch 2/10\n9288/9288 [==============================] - 6s 598us/sample - loss: 0.5929 - val_loss: 0.6560\nEpoch 3/10\n9288/9288 [==============================] - 5s 537us/sample - loss: 0.5049 - val_loss: 0.5861\nEpoch 4/10\n9288/9288 [==============================] - 5s 536us/sample - loss: 0.4470 - val_loss: 0.5520\nEpoch 5/10\n9288/9288 [==============================] - 4s 469us/sample - loss: 0.4104 - val_loss: 0.5128\nEpoch 6/10\n9288/9288 [==============================] - 4s 424us/sample - loss: 0.3905 - val_loss: 0.5050\nEpoch 7/10\n9288/9288 [==============================] - 5s 580us/sample - loss: 0.3771 - val_loss: 0.4947\nEpoch 8/10\n9288/9288 [==============================] - 4s 434us/sample - loss: 0.3692 - val_loss: 0.5095\nEpoch 9/10\n9288/9288 [==============================] - 5s 555us/sample - loss: 0.3622 - val_loss: 0.4900\nEpoch 10/10\n9288/9288 [==============================] - 5s 526us/sample - loss: 0.3574 - val_loss: 0.4917\n2322/2322 [==============================] - 0s 188us/sample - loss: 0.3850\n9288/9288 [==============================] - 2s 191us/sample - loss: 0.3783\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 8s 893us/sample - loss: 0.9724 - val_loss: 0.6581\nEpoch 2/10\n9288/9288 [==============================] - 7s 758us/sample - loss: 0.4715 - val_loss: 0.5589\nEpoch 3/10\n9288/9288 [==============================] - 5s 490us/sample - loss: 0.4178 - val_loss: 0.5267\nEpoch 4/10\n9288/9288 [==============================] - 6s 695us/sample - loss: 0.3967 - val_loss: 0.5101\nEpoch 5/10\n9288/9288 [==============================] - 4s 437us/sample - loss: 0.3835 - val_loss: 0.4954\nEpoch 6/10\n9288/9288 [==============================] - 5s 495us/sample - loss: 0.3735 - val_loss: 0.5085\nEpoch 7/10\n9288/9288 [==============================] - 7s 760us/sample - loss: 0.3668 - val_loss: 0.5118\nEpoch 8/10\n9288/9288 [==============================] - 4s 452us/sample - loss: 0.3614 - val_loss: 0.4829\nEpoch 9/10\n9288/9288 [==============================] - 14s 1ms/sample - loss: 0.3561 - val_loss: 0.4754\nEpoch 10/10\n9288/9288 [==============================] - 4s 442us/sample - loss: 0.3524 - val_loss: 0.4794\n2322/2322 [==============================] - 1s 526us/sample - loss: 0.3789\n9288/9288 [==============================] - 2s 178us/sample - loss: 0.3492\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 8s 865us/sample - loss: 1.0190 - val_loss: 0.6691\nEpoch 2/10\n9288/9288 [==============================] - 8s 810us/sample - loss: 0.5133 - val_loss: 0.5875\nEpoch 3/10\n9288/9288 [==============================] - 6s 602us/sample - loss: 0.4499 - val_loss: 0.5401\nEpoch 4/10\n9288/9288 [==============================] - 9s 985us/sample - loss: 0.4157 - val_loss: 0.5250\nEpoch 5/10\n9288/9288 [==============================] - 5s 501us/sample - loss: 0.3952 - val_loss: 0.5056\nEpoch 6/10\n9288/9288 [==============================] - 6s 639us/sample - loss: 0.3815 - val_loss: 0.4948\nEpoch 7/10\n9288/9288 [==============================] - 4s 430us/sample - loss: 0.3719 - val_loss: 0.4786\nEpoch 8/10\n9288/9288 [==============================] - 6s 631us/sample - loss: 0.3637 - val_loss: 0.4781\nEpoch 9/10\n9288/9288 [==============================] - 5s 516us/sample - loss: 0.3578 - val_loss: 0.4765\nEpoch 10/10\n9288/9288 [==============================] - 6s 678us/sample - loss: 0.3534 - val_loss: 0.4858\n2322/2322 [==============================] - 0s 196us/sample - loss: 0.3649\n9288/9288 [==============================] - 2s 192us/sample - loss: 0.3504\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 7s 756us/sample - loss: 0.9334 - val_loss: 0.6960\nEpoch 2/10\n9288/9288 [==============================] - 4s 458us/sample - loss: 0.5183 - val_loss: 0.5856\nEpoch 3/10\n9288/9288 [==============================] - 4s 446us/sample - loss: 0.4322 - val_loss: 0.5219\nEpoch 4/10\n9288/9288 [==============================] - 10s 1ms/sample - loss: 0.3971 - val_loss: 0.4981\nEpoch 5/10\n9288/9288 [==============================] - 7s 754us/sample - loss: 0.3814 - val_loss: 0.4958\nEpoch 6/10\n9288/9288 [==============================] - 5s 536us/sample - loss: 0.3720 - val_loss: 0.4842\nEpoch 7/10\n9288/9288 [==============================] - 6s 649us/sample - loss: 0.3632 - val_loss: 0.4794\nEpoch 8/10\n9288/9288 [==============================] - 4s 430us/sample - loss: 0.3619 - val_loss: 0.4795\nEpoch 9/10\n9288/9288 [==============================] - 7s 745us/sample - loss: 0.3563 - val_loss: 0.4769\nEpoch 10/10\n9288/9288 [==============================] - 5s 486us/sample - loss: 0.3521 - val_loss: 0.4716\n2322/2322 [==============================] - 0s 187us/sample - loss: 0.3529\n9288/9288 [==============================] - 2s 178us/sample - loss: 0.3483\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 7s 706us/sample - loss: 0.9792 - val_loss: 0.7304\nEpoch 2/10\n9288/9288 [==============================] - 4s 448us/sample - loss: 0.5903 - val_loss: 0.6396\nEpoch 3/10\n9288/9288 [==============================] - 5s 580us/sample - loss: 0.5051 - val_loss: 0.5705\nEpoch 4/10\n9288/9288 [==============================] - 5s 489us/sample - loss: 0.4494 - val_loss: 0.5366\nEpoch 5/10\n9288/9288 [==============================] - 6s 650us/sample - loss: 0.4203 - val_loss: 0.5121\nEpoch 6/10\n9288/9288 [==============================] - 4s 471us/sample - loss: 0.3992 - val_loss: 0.5105\nEpoch 7/10\n",
                    "name": "stdout"
                },
                {
                    "output_type": "stream",
                    "text": "9288/9288 [==============================] - 6s 597us/sample - loss: 0.3840 - val_loss: 0.5005\nEpoch 8/10\n9288/9288 [==============================] - 4s 448us/sample - loss: 0.3776 - val_loss: 0.4892\nEpoch 9/10\n9288/9288 [==============================] - 4s 480us/sample - loss: 0.3712 - val_loss: 0.5043\nEpoch 10/10\n9288/9288 [==============================] - 5s 560us/sample - loss: 0.3658 - val_loss: 0.4842\n2322/2322 [==============================] - 0s 188us/sample - loss: 0.3522\n9288/9288 [==============================] - 2s 181us/sample - loss: 0.3630\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 2s 173us/sample - loss: 4.0084 - val_loss: 1.8449\nEpoch 2/10\n9288/9288 [==============================] - 0s 38us/sample - loss: 1.1367 - val_loss: 0.9117\nEpoch 3/10\n9288/9288 [==============================] - 0s 39us/sample - loss: 0.7001 - val_loss: 0.7121\nEpoch 4/10\n9288/9288 [==============================] - 0s 40us/sample - loss: 0.6111 - val_loss: 0.6554\nEpoch 5/10\n9288/9288 [==============================] - 0s 41us/sample - loss: 0.5864 - val_loss: 0.6339\nEpoch 6/10\n9288/9288 [==============================] - 0s 39us/sample - loss: 0.5768 - val_loss: 0.6242\nEpoch 7/10\n9288/9288 [==============================] - 0s 40us/sample - loss: 0.5717 - val_loss: 0.6175\nEpoch 8/10\n9288/9288 [==============================] - 0s 40us/sample - loss: 0.5679 - val_loss: 0.6131\nEpoch 9/10\n9288/9288 [==============================] - 0s 40us/sample - loss: 0.5626 - val_loss: 0.6080\nEpoch 10/10\n9288/9288 [==============================] - 0s 40us/sample - loss: 0.5609 - val_loss: 0.6031\n2322/2322 [==============================] - 0s 19us/sample - loss: 0.5347\n9288/9288 [==============================] - 0s 19us/sample - loss: 0.5537\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 2s 180us/sample - loss: 3.2484 - val_loss: 1.9468\nEpoch 2/10\n9288/9288 [==============================] - 0s 39us/sample - loss: 1.1380 - val_loss: 1.0022\nEpoch 3/10\n9288/9288 [==============================] - 0s 41us/sample - loss: 0.7165 - val_loss: 0.7511\nEpoch 4/10\n9288/9288 [==============================] - 0s 40us/sample - loss: 0.6115 - val_loss: 0.6593\nEpoch 5/10\n9288/9288 [==============================] - 0s 41us/sample - loss: 0.5728 - val_loss: 0.6165\nEpoch 6/10\n9288/9288 [==============================] - 0s 40us/sample - loss: 0.5520 - val_loss: 0.5975\nEpoch 7/10\n9288/9288 [==============================] - 0s 40us/sample - loss: 0.5382 - val_loss: 0.5904\nEpoch 8/10\n9288/9288 [==============================] - 0s 39us/sample - loss: 0.5285 - val_loss: 0.5906\nEpoch 9/10\n9288/9288 [==============================] - 0s 39us/sample - loss: 0.5215 - val_loss: 0.5935\nEpoch 10/10\n9288/9288 [==============================] - 0s 41us/sample - loss: 0.5163 - val_loss: 0.5983\n2322/2322 [==============================] - 0s 19us/sample - loss: 0.6531\n9288/9288 [==============================] - 0s 20us/sample - loss: 0.5134\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 2s 186us/sample - loss: 4.0856 - val_loss: 1.7081\nEpoch 2/10\n9288/9288 [==============================] - 0s 40us/sample - loss: 1.0956 - val_loss: 0.8238\nEpoch 3/10\n9288/9288 [==============================] - 0s 39us/sample - loss: 0.6576 - val_loss: 0.6615\nEpoch 4/10\n9288/9288 [==============================] - 0s 39us/sample - loss: 0.5732 - val_loss: 0.6244\nEpoch 5/10\n9288/9288 [==============================] - 0s 39us/sample - loss: 0.5550 - val_loss: 0.6102\nEpoch 6/10\n9288/9288 [==============================] - 0s 40us/sample - loss: 0.5452 - val_loss: 0.6018\nEpoch 7/10\n9288/9288 [==============================] - 0s 40us/sample - loss: 0.5423 - val_loss: 0.5948\nEpoch 8/10\n9288/9288 [==============================] - 0s 41us/sample - loss: 0.5353 - val_loss: 0.5896\nEpoch 9/10\n9288/9288 [==============================] - 0s 41us/sample - loss: 0.5359 - val_loss: 0.5844\nEpoch 10/10\n9288/9288 [==============================] - 0s 40us/sample - loss: 0.5293 - val_loss: 0.5795\n2322/2322 [==============================] - 0s 20us/sample - loss: 0.5448\n9288/9288 [==============================] - 0s 20us/sample - loss: 0.5259\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 2s 201us/sample - loss: 4.3380 - val_loss: 2.0333\nEpoch 2/10\n9288/9288 [==============================] - 0s 40us/sample - loss: 1.2594 - val_loss: 0.9984\nEpoch 3/10\n9288/9288 [==============================] - 0s 40us/sample - loss: 0.7577 - val_loss: 0.7544\nEpoch 4/10\n9288/9288 [==============================] - 0s 41us/sample - loss: 0.6427 - val_loss: 0.6823\nEpoch 5/10\n9288/9288 [==============================] - 0s 41us/sample - loss: 0.6057 - val_loss: 0.6553\nEpoch 6/10\n9288/9288 [==============================] - 0s 41us/sample - loss: 0.5927 - val_loss: 0.6416\nEpoch 7/10\n9288/9288 [==============================] - 0s 41us/sample - loss: 0.5839 - val_loss: 0.6338\nEpoch 8/10\n9288/9288 [==============================] - 0s 40us/sample - loss: 0.5815 - val_loss: 0.6267\nEpoch 9/10\n9288/9288 [==============================] - 0s 40us/sample - loss: 0.5715 - val_loss: 0.6203\nEpoch 10/10\n9288/9288 [==============================] - 0s 41us/sample - loss: 0.5712 - val_loss: 0.6142\n2322/2322 [==============================] - 0s 23us/sample - loss: 0.5515\n9288/9288 [==============================] - 0s 20us/sample - loss: 0.5629\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 2s 201us/sample - loss: 3.8049 - val_loss: 1.9882\nEpoch 2/10\n9288/9288 [==============================] - 0s 43us/sample - loss: 1.2103 - val_loss: 0.9277\nEpoch 3/10\n9288/9288 [==============================] - 0s 41us/sample - loss: 0.7048 - val_loss: 0.6895\nEpoch 4/10\n9288/9288 [==============================] - 0s 39us/sample - loss: 0.5932 - val_loss: 0.6276\nEpoch 5/10\n9288/9288 [==============================] - 0s 39us/sample - loss: 0.5638 - val_loss: 0.6065\nEpoch 6/10\n9288/9288 [==============================] - 0s 39us/sample - loss: 0.5534 - val_loss: 0.5962\nEpoch 7/10\n9288/9288 [==============================] - 0s 39us/sample - loss: 0.5493 - val_loss: 0.5900\nEpoch 8/10\n9288/9288 [==============================] - 0s 40us/sample - loss: 0.5476 - val_loss: 0.5851\nEpoch 9/10\n9288/9288 [==============================] - 0s 39us/sample - loss: 0.5401 - val_loss: 0.5807\nEpoch 10/10\n9288/9288 [==============================] - 0s 40us/sample - loss: 0.5413 - val_loss: 0.5778\n2322/2322 [==============================] - 0s 20us/sample - loss: 0.5254\n9288/9288 [==============================] - 0s 20us/sample - loss: 0.5365\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 4s 479us/sample - loss: 3.2196 - val_loss: 2.0690\nEpoch 2/10\n9288/9288 [==============================] - 4s 390us/sample - loss: 1.6017 - val_loss: 1.2528\nEpoch 3/10\n9288/9288 [==============================] - 4s 471us/sample - loss: 1.0590 - val_loss: 0.9775\nEpoch 4/10\n9288/9288 [==============================] - 3s 300us/sample - loss: 0.8629 - val_loss: 0.8911\nEpoch 5/10\n9288/9288 [==============================] - 4s 386us/sample - loss: 0.7971 - val_loss: 0.8547\nEpoch 6/10\n9288/9288 [==============================] - 5s 510us/sample - loss: 0.7668 - val_loss: 0.8332\nEpoch 7/10\n9288/9288 [==============================] - 3s 302us/sample - loss: 0.7465 - val_loss: 0.8175\nEpoch 8/10\n9288/9288 [==============================] - 3s 304us/sample - loss: 0.7308 - val_loss: 0.8029\nEpoch 9/10\n9288/9288 [==============================] - 3s 325us/sample - loss: 0.7170 - val_loss: 0.7900\nEpoch 10/10\n9288/9288 [==============================] - 5s 510us/sample - loss: 0.7048 - val_loss: 0.7783\n2322/2322 [==============================] - 0s 130us/sample - loss: 0.6881\n9288/9288 [==============================] - 1s 125us/sample - loss: 0.6981\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n",
                    "name": "stdout"
                },
                {
                    "output_type": "stream",
                    "text": "9288/9288 [==============================] - 4s 468us/sample - loss: 4.6447 - val_loss: 3.0769\nEpoch 2/10\n9288/9288 [==============================] - 5s 513us/sample - loss: 2.1287 - val_loss: 1.6862\nEpoch 3/10\n9288/9288 [==============================] - 3s 301us/sample - loss: 1.3247 - val_loss: 1.2537\nEpoch 4/10\n9288/9288 [==============================] - 3s 323us/sample - loss: 1.0957 - val_loss: 1.1131\nEpoch 5/10\n9288/9288 [==============================] - 4s 442us/sample - loss: 0.9973 - val_loss: 1.0342\nEpoch 6/10\n9288/9288 [==============================] - 3s 307us/sample - loss: 0.9308 - val_loss: 0.9792\nEpoch 7/10\n9288/9288 [==============================] - 3s 306us/sample - loss: 0.8809 - val_loss: 0.9368\nEpoch 8/10\n9288/9288 [==============================] - 3s 323us/sample - loss: 0.8413 - val_loss: 0.9036\nEpoch 9/10\n9288/9288 [==============================] - 4s 437us/sample - loss: 0.8085 - val_loss: 0.8759\nEpoch 10/10\n9288/9288 [==============================] - 3s 299us/sample - loss: 0.7805 - val_loss: 0.8524\n2322/2322 [==============================] - 0s 137us/sample - loss: 0.7963\n9288/9288 [==============================] - 1s 127us/sample - loss: 0.7671\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 7s 704us/sample - loss: 3.9933 - val_loss: 2.2441\nEpoch 2/10\n9288/9288 [==============================] - 3s 297us/sample - loss: 1.5722 - val_loss: 1.2237\nEpoch 3/10\n9288/9288 [==============================] - 3s 308us/sample - loss: 1.0049 - val_loss: 0.9660\nEpoch 4/10\n9288/9288 [==============================] - 4s 426us/sample - loss: 0.8210 - val_loss: 0.8635\nEpoch 5/10\n9288/9288 [==============================] - 3s 328us/sample - loss: 0.7327 - val_loss: 0.8104\nEpoch 6/10\n9288/9288 [==============================] - 3s 318us/sample - loss: 0.6799 - val_loss: 0.7766\nEpoch 7/10\n9288/9288 [==============================] - 3s 308us/sample - loss: 0.6448 - val_loss: 0.7529\nEpoch 8/10\n9288/9288 [==============================] - 4s 424us/sample - loss: 0.6190 - val_loss: 0.7335\nEpoch 9/10\n9288/9288 [==============================] - 3s 303us/sample - loss: 0.5982 - val_loss: 0.7163\nEpoch 10/10\n9288/9288 [==============================] - 3s 306us/sample - loss: 0.5800 - val_loss: 0.6998\n2322/2322 [==============================] - 0s 128us/sample - loss: 0.6038\n9288/9288 [==============================] - 1s 128us/sample - loss: 0.5710\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 5s 562us/sample - loss: 1.9813 - val_loss: 1.3689\nEpoch 2/10\n9288/9288 [==============================] - 3s 310us/sample - loss: 1.0011 - val_loss: 1.0225\nEpoch 3/10\n9288/9288 [==============================] - 3s 303us/sample - loss: 0.8486 - val_loss: 0.9151\nEpoch 4/10\n9288/9288 [==============================] - 5s 501us/sample - loss: 0.7628 - val_loss: 0.8617\nEpoch 5/10\n9288/9288 [==============================] - 3s 353us/sample - loss: 0.7160 - val_loss: 0.8275\nEpoch 6/10\n9288/9288 [==============================] - 3s 308us/sample - loss: 0.6819 - val_loss: 0.8013\nEpoch 7/10\n9288/9288 [==============================] - 4s 461us/sample - loss: 0.6546 - val_loss: 0.7793\nEpoch 8/10\n9288/9288 [==============================] - 3s 370us/sample - loss: 0.6319 - val_loss: 0.7603\nEpoch 9/10\n9288/9288 [==============================] - 3s 302us/sample - loss: 0.6120 - val_loss: 0.7435\nEpoch 10/10\n9288/9288 [==============================] - 4s 432us/sample - loss: 0.5938 - val_loss: 0.7275\n2322/2322 [==============================] - 0s 121us/sample - loss: 0.5912\n9288/9288 [==============================] - 1s 129us/sample - loss: 0.5844\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 5s 535us/sample - loss: 3.7316 - val_loss: 2.4998\nEpoch 2/10\n9288/9288 [==============================] - 3s 309us/sample - loss: 1.8226 - val_loss: 1.6321\nEpoch 3/10\n9288/9288 [==============================] - 4s 481us/sample - loss: 1.2792 - val_loss: 1.2759\nEpoch 4/10\n9288/9288 [==============================] - 3s 309us/sample - loss: 1.0361 - val_loss: 1.0820\nEpoch 5/10\n9288/9288 [==============================] - 3s 298us/sample - loss: 0.9028 - val_loss: 0.9689\nEpoch 6/10\n9288/9288 [==============================] - 4s 456us/sample - loss: 0.8260 - val_loss: 0.9027\nEpoch 7/10\n9288/9288 [==============================] - 3s 301us/sample - loss: 0.7812 - val_loss: 0.8627\nEpoch 8/10\n9288/9288 [==============================] - 3s 301us/sample - loss: 0.7532 - val_loss: 0.8367\nEpoch 9/10\n9288/9288 [==============================] - 3s 304us/sample - loss: 0.7323 - val_loss: 0.8156\nEpoch 10/10\n9288/9288 [==============================] - 4s 417us/sample - loss: 0.7153 - val_loss: 0.7996\n2322/2322 [==============================] - 0s 129us/sample - loss: 0.6795\n9288/9288 [==============================] - 1s 134us/sample - loss: 0.7068\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 5s 551us/sample - loss: 1.0838 - val_loss: 0.8154\nEpoch 2/10\n9288/9288 [==============================] - 4s 390us/sample - loss: 0.6428 - val_loss: 0.7284\nEpoch 3/10\n9288/9288 [==============================] - 2s 267us/sample - loss: 0.5636 - val_loss: 0.6716\nEpoch 4/10\n9288/9288 [==============================] - 3s 274us/sample - loss: 0.5260 - val_loss: 0.6327\nEpoch 5/10\n9288/9288 [==============================] - 3s 270us/sample - loss: 0.5006 - val_loss: 0.6086\nEpoch 6/10\n9288/9288 [==============================] - 4s 401us/sample - loss: 0.4783 - val_loss: 0.5736\nEpoch 7/10\n9288/9288 [==============================] - 3s 334us/sample - loss: 0.4596 - val_loss: 0.5579\nEpoch 8/10\n9288/9288 [==============================] - 3s 342us/sample - loss: 0.4441 - val_loss: 0.5449\nEpoch 9/10\n9288/9288 [==============================] - 4s 411us/sample - loss: 0.4309 - val_loss: 0.5191\nEpoch 10/10\n9288/9288 [==============================] - 3s 327us/sample - loss: 0.4195 - val_loss: 0.5161\n2322/2322 [==============================] - 0s 120us/sample - loss: 0.4086\n9288/9288 [==============================] - 1s 116us/sample - loss: 0.4143\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 4s 474us/sample - loss: 2.0019 - val_loss: 1.4020\nEpoch 2/10\n9288/9288 [==============================] - 4s 411us/sample - loss: 1.2928 - val_loss: 1.3698\nEpoch 3/10\n9288/9288 [==============================] - 3s 275us/sample - loss: 1.2624 - val_loss: 1.3109\nEpoch 4/10\n9288/9288 [==============================] - 3s 274us/sample - loss: 1.1726 - val_loss: 1.1593\nEpoch 5/10\n9288/9288 [==============================] - 3s 306us/sample - loss: 0.9822 - val_loss: 0.9362\nEpoch 6/10\n9288/9288 [==============================] - 4s 442us/sample - loss: 0.8192 - val_loss: 0.8216\nEpoch 7/10\n9288/9288 [==============================] - 3s 271us/sample - loss: 0.7191 - val_loss: 0.7419\nEpoch 8/10\n9288/9288 [==============================] - 3s 272us/sample - loss: 0.6352 - val_loss: 0.7387\nEpoch 9/10\n9288/9288 [==============================] - 4s 473us/sample - loss: 0.5885 - val_loss: 0.6539\nEpoch 10/10\n9288/9288 [==============================] - 3s 273us/sample - loss: 0.5621 - val_loss: 0.6580\n2322/2322 [==============================] - 0s 115us/sample - loss: 0.8475\n9288/9288 [==============================] - 1s 117us/sample - loss: 0.5419\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 4s 484us/sample - loss: 1.6219 - val_loss: 0.7636\nEpoch 2/10\n9288/9288 [==============================] - 4s 413us/sample - loss: 0.5965 - val_loss: 0.6695\nEpoch 3/10\n9288/9288 [==============================] - 3s 273us/sample - loss: 0.5315 - val_loss: 0.6283\nEpoch 4/10\n9288/9288 [==============================] - 3s 277us/sample - loss: 0.4878 - val_loss: 0.5929\nEpoch 5/10\n9288/9288 [==============================] - 3s 285us/sample - loss: 0.4580 - val_loss: 0.5637\nEpoch 6/10\n9288/9288 [==============================] - 4s 450us/sample - loss: 0.4401 - val_loss: 0.5454\nEpoch 7/10\n",
                    "name": "stdout"
                },
                {
                    "output_type": "stream",
                    "text": "9288/9288 [==============================] - 3s 276us/sample - loss: 0.4278 - val_loss: 0.5270\nEpoch 8/10\n9288/9288 [==============================] - 3s 275us/sample - loss: 0.4174 - val_loss: 0.5170\nEpoch 9/10\n9288/9288 [==============================] - 3s 330us/sample - loss: 0.4121 - val_loss: 0.5090\nEpoch 10/10\n9288/9288 [==============================] - 3s 320us/sample - loss: 0.4119 - val_loss: 0.5124\n2322/2322 [==============================] - 0s 119us/sample - loss: 0.4405\n9288/9288 [==============================] - 1s 116us/sample - loss: 0.4117\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 5s 508us/sample - loss: 1.4421 - val_loss: 0.8113\nEpoch 2/10\n9288/9288 [==============================] - 4s 402us/sample - loss: 0.6512 - val_loss: 0.6449\nEpoch 3/10\n9288/9288 [==============================] - 3s 273us/sample - loss: 0.5651 - val_loss: 0.6088\nEpoch 4/10\n9288/9288 [==============================] - 3s 290us/sample - loss: 0.5237 - val_loss: 0.5858\nEpoch 5/10\n9288/9288 [==============================] - 4s 444us/sample - loss: 0.4946 - val_loss: 0.5656\nEpoch 6/10\n9288/9288 [==============================] - 3s 273us/sample - loss: 0.4739 - val_loss: 0.5446\nEpoch 7/10\n9288/9288 [==============================] - 3s 290us/sample - loss: 0.4565 - val_loss: 0.5287\nEpoch 8/10\n9288/9288 [==============================] - 2s 266us/sample - loss: 0.4432 - val_loss: 0.5070\nEpoch 9/10\n9288/9288 [==============================] - 4s 382us/sample - loss: 0.4323 - val_loss: 0.5029\nEpoch 10/10\n9288/9288 [==============================] - 3s 299us/sample - loss: 0.4231 - val_loss: 0.5014\n2322/2322 [==============================] - 0s 114us/sample - loss: 0.4240\n9288/9288 [==============================] - 1s 116us/sample - loss: 0.4182\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 5s 548us/sample - loss: 1.7441 - val_loss: 1.1052\nEpoch 2/10\n9288/9288 [==============================] - 4s 415us/sample - loss: 0.7612 - val_loss: 0.6221\nEpoch 3/10\n9288/9288 [==============================] - 3s 273us/sample - loss: 0.4824 - val_loss: 0.5202\nEpoch 4/10\n9288/9288 [==============================] - 3s 278us/sample - loss: 0.4314 - val_loss: 0.5103\nEpoch 5/10\n9288/9288 [==============================] - 3s 364us/sample - loss: 0.4181 - val_loss: 0.4944\nEpoch 6/10\n9288/9288 [==============================] - 3s 331us/sample - loss: 0.4103 - val_loss: 0.4869\nEpoch 7/10\n9288/9288 [==============================] - 3s 274us/sample - loss: 0.4072 - val_loss: 0.4960\nEpoch 8/10\n9288/9288 [==============================] - 3s 269us/sample - loss: 0.4027 - val_loss: 0.5024\nEpoch 9/10\n9288/9288 [==============================] - 4s 444us/sample - loss: 0.4011 - val_loss: 0.4985\nEpoch 10/10\n9288/9288 [==============================] - 3s 277us/sample - loss: 0.4008 - val_loss: 0.4939\n2322/2322 [==============================] - 0s 114us/sample - loss: 0.3736\n9288/9288 [==============================] - 1s 118us/sample - loss: 0.4012\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 2s 269us/sample - loss: 1.1719 - val_loss: 0.6574\nEpoch 2/10\n9288/9288 [==============================] - 0s 43us/sample - loss: 0.8265 - val_loss: 0.5911\nEpoch 3/10\n9288/9288 [==============================] - 0s 46us/sample - loss: 1.5421 - val_loss: 1.9141\nEpoch 4/10\n9288/9288 [==============================] - 0s 45us/sample - loss: 6.0247 - val_loss: 0.6467\nEpoch 5/10\n9288/9288 [==============================] - 0s 46us/sample - loss: 1.7852 - val_loss: 0.6144\nEpoch 6/10\n9288/9288 [==============================] - 0s 44us/sample - loss: 1.0387 - val_loss: 3.7753\nEpoch 7/10\n9288/9288 [==============================] - 0s 45us/sample - loss: 4.4353 - val_loss: 0.6617\nEpoch 8/10\n9288/9288 [==============================] - 0s 44us/sample - loss: 1.9674 - val_loss: 0.5553\nEpoch 9/10\n9288/9288 [==============================] - 0s 44us/sample - loss: 0.8269 - val_loss: 10.3680\nEpoch 10/10\n9288/9288 [==============================] - 0s 47us/sample - loss: 4.6693 - val_loss: 0.7431\n2322/2322 [==============================] - 0s 25us/sample - loss: 0.4821\n9288/9288 [==============================] - 0s 25us/sample - loss: 0.7224\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 3s 272us/sample - loss: 0.7796 - val_loss: 0.6641\nEpoch 2/10\n9288/9288 [==============================] - 0s 45us/sample - loss: 0.5076 - val_loss: 0.6392\nEpoch 3/10\n9288/9288 [==============================] - 0s 47us/sample - loss: 0.5025 - val_loss: 0.5512\nEpoch 4/10\n9288/9288 [==============================] - 0s 45us/sample - loss: 0.4869 - val_loss: 0.7409\nEpoch 5/10\n9288/9288 [==============================] - 0s 46us/sample - loss: 0.5087 - val_loss: 0.6535\nEpoch 6/10\n9288/9288 [==============================] - 0s 45us/sample - loss: 0.5019 - val_loss: 0.7126\nEpoch 7/10\n9288/9288 [==============================] - 0s 46us/sample - loss: 0.5038 - val_loss: 0.6745\nEpoch 8/10\n9288/9288 [==============================] - 0s 47us/sample - loss: 0.4999 - val_loss: 0.7408\nEpoch 9/10\n9288/9288 [==============================] - 0s 44us/sample - loss: 0.5090 - val_loss: 0.6843\nEpoch 10/10\n9288/9288 [==============================] - 0s 44us/sample - loss: 0.5047 - val_loss: 0.6730\n2322/2322 [==============================] - 0s 23us/sample - loss: 1.1268\n9288/9288 [==============================] - 0s 23us/sample - loss: 0.4898\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 3s 279us/sample - loss: 1.5378 - val_loss: 4.6258\nEpoch 2/10\n9288/9288 [==============================] - 0s 45us/sample - loss: 0.9598 - val_loss: 2.4950\nEpoch 3/10\n9288/9288 [==============================] - 0s 44us/sample - loss: 0.8638 - val_loss: 0.6148\nEpoch 4/10\n9288/9288 [==============================] - 0s 45us/sample - loss: 1.3014 - val_loss: 21.7050\nEpoch 5/10\n9288/9288 [==============================] - 0s 43us/sample - loss: 1.6837 - val_loss: 0.9203\nEpoch 6/10\n9288/9288 [==============================] - 0s 45us/sample - loss: 0.7602 - val_loss: 0.6235\nEpoch 7/10\n9288/9288 [==============================] - 0s 46us/sample - loss: 1.8013 - val_loss: 0.5662\nEpoch 8/10\n9288/9288 [==============================] - 0s 44us/sample - loss: 0.9405 - val_loss: 0.5560\nEpoch 9/10\n9288/9288 [==============================] - 0s 43us/sample - loss: 0.6165 - val_loss: 0.6033\nEpoch 10/10\n9288/9288 [==============================] - 0s 45us/sample - loss: 1.5972 - val_loss: 1.1762\n2322/2322 [==============================] - 0s 23us/sample - loss: 1.0735\n9288/9288 [==============================] - 0s 24us/sample - loss: 1.2846\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 3s 280us/sample - loss: 0.8920 - val_loss: 0.7256\nEpoch 2/10\n9288/9288 [==============================] - 0s 44us/sample - loss: 2.6798 - val_loss: 2.2345\nEpoch 3/10\n9288/9288 [==============================] - 0s 45us/sample - loss: 8.1672 - val_loss: 3.3132\nEpoch 4/10\n9288/9288 [==============================] - 0s 45us/sample - loss: 5.8677 - val_loss: 8.5160\nEpoch 5/10\n9288/9288 [==============================] - 0s 45us/sample - loss: 0.9082 - val_loss: 0.5765\nEpoch 6/10\n9288/9288 [==============================] - 0s 44us/sample - loss: 1.3611 - val_loss: 0.5992\nEpoch 7/10\n9288/9288 [==============================] - 0s 44us/sample - loss: 1.0601 - val_loss: 8.6410\nEpoch 8/10\n9288/9288 [==============================] - 0s 44us/sample - loss: 11.2557 - val_loss: 8.6016\nEpoch 9/10\n9288/9288 [==============================] - 0s 45us/sample - loss: 41.9241 - val_loss: 10.4920\nEpoch 10/10\n9288/9288 [==============================] - 0s 44us/sample - loss: 5.0866 - val_loss: 8.1613\n2322/2322 [==============================] - 0s 24us/sample - loss: 3.6301\n9288/9288 [==============================] - 0s 24us/sample - loss: 9.0302\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n",
                    "name": "stdout"
                },
                {
                    "output_type": "stream",
                    "text": "9288/9288 [==============================] - 3s 291us/sample - loss: 1.6574 - val_loss: 1.4741\nEpoch 2/10\n9288/9288 [==============================] - 0s 46us/sample - loss: 0.9776 - val_loss: 0.7623\nEpoch 3/10\n9288/9288 [==============================] - 0s 45us/sample - loss: 3.0743 - val_loss: 0.6128\nEpoch 4/10\n9288/9288 [==============================] - 0s 45us/sample - loss: 0.7979 - val_loss: 3.1851\nEpoch 5/10\n9288/9288 [==============================] - 0s 44us/sample - loss: 0.7910 - val_loss: 0.6879\nEpoch 6/10\n9288/9288 [==============================] - 0s 43us/sample - loss: 1.6561 - val_loss: 0.5610\nEpoch 7/10\n9288/9288 [==============================] - 0s 45us/sample - loss: 0.6628 - val_loss: 0.6552\nEpoch 8/10\n9288/9288 [==============================] - 0s 45us/sample - loss: 1.5561 - val_loss: 0.5585\nEpoch 9/10\n9288/9288 [==============================] - 0s 45us/sample - loss: 0.6519 - val_loss: 0.5741\nEpoch 10/10\n9288/9288 [==============================] - 0s 44us/sample - loss: 0.5538 - val_loss: 0.6576\n2322/2322 [==============================] - 0s 28us/sample - loss: 0.4761\n9288/9288 [==============================] - 0s 24us/sample - loss: 0.6182\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 3s 296us/sample - loss: 4.3930 - val_loss: 2.1776\nEpoch 2/10\n9288/9288 [==============================] - 0s 46us/sample - loss: 1.4082 - val_loss: 1.0869\nEpoch 3/10\n9288/9288 [==============================] - 0s 46us/sample - loss: 0.8468 - val_loss: 0.8389\nEpoch 4/10\n9288/9288 [==============================] - 0s 47us/sample - loss: 0.7200 - val_loss: 0.7648\nEpoch 5/10\n9288/9288 [==============================] - 0s 45us/sample - loss: 0.6766 - val_loss: 0.7306\nEpoch 6/10\n9288/9288 [==============================] - 0s 46us/sample - loss: 0.6560 - val_loss: 0.7087\nEpoch 7/10\n9288/9288 [==============================] - 0s 46us/sample - loss: 0.6426 - val_loss: 0.6910\nEpoch 8/10\n9288/9288 [==============================] - 0s 45us/sample - loss: 0.6283 - val_loss: 0.6759\nEpoch 9/10\n9288/9288 [==============================] - 0s 47us/sample - loss: 0.6163 - val_loss: 0.6628\nEpoch 10/10\n9288/9288 [==============================] - 0s 46us/sample - loss: 0.6040 - val_loss: 0.6515\n2322/2322 [==============================] - 0s 25us/sample - loss: 0.5850\n9288/9288 [==============================] - 0s 25us/sample - loss: 0.5975\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 3s 304us/sample - loss: 3.1244 - val_loss: 1.8354\nEpoch 2/10\n9288/9288 [==============================] - 0s 45us/sample - loss: 1.1306 - val_loss: 0.9247\nEpoch 3/10\n9288/9288 [==============================] - 0s 46us/sample - loss: 0.6992 - val_loss: 0.6931\nEpoch 4/10\n9288/9288 [==============================] - 0s 46us/sample - loss: 0.5902 - val_loss: 0.6225\nEpoch 5/10\n9288/9288 [==============================] - 0s 46us/sample - loss: 0.5534 - val_loss: 0.5973\nEpoch 6/10\n9288/9288 [==============================] - 0s 48us/sample - loss: 0.5357 - val_loss: 0.5894\nEpoch 7/10\n9288/9288 [==============================] - 0s 47us/sample - loss: 0.5244 - val_loss: 0.5892\nEpoch 8/10\n9288/9288 [==============================] - 0s 47us/sample - loss: 0.5166 - val_loss: 0.5927\nEpoch 9/10\n9288/9288 [==============================] - 0s 47us/sample - loss: 0.5108 - val_loss: 0.5984\nEpoch 10/10\n9288/9288 [==============================] - 0s 49us/sample - loss: 0.5063 - val_loss: 0.6059\n2322/2322 [==============================] - 0s 24us/sample - loss: 0.7131\n9288/9288 [==============================] - 0s 25us/sample - loss: 0.5040\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 3s 306us/sample - loss: 3.6017 - val_loss: 1.8169\nEpoch 2/10\n9288/9288 [==============================] - 0s 47us/sample - loss: 1.2040 - val_loss: 0.9827\nEpoch 3/10\n9288/9288 [==============================] - 0s 47us/sample - loss: 0.7624 - val_loss: 0.7905\nEpoch 4/10\n9288/9288 [==============================] - 0s 46us/sample - loss: 0.6589 - val_loss: 0.7321\nEpoch 5/10\n9288/9288 [==============================] - 0s 48us/sample - loss: 0.6270 - val_loss: 0.7046\nEpoch 6/10\n9288/9288 [==============================] - 0s 46us/sample - loss: 0.6093 - val_loss: 0.6848\nEpoch 7/10\n9288/9288 [==============================] - 0s 46us/sample - loss: 0.5952 - val_loss: 0.6685\nEpoch 8/10\n9288/9288 [==============================] - 0s 45us/sample - loss: 0.5873 - val_loss: 0.6542\nEpoch 9/10\n9288/9288 [==============================] - 0s 45us/sample - loss: 0.5766 - val_loss: 0.6416\nEpoch 10/10\n9288/9288 [==============================] - 0s 44us/sample - loss: 0.5697 - val_loss: 0.6307\n2322/2322 [==============================] - 0s 25us/sample - loss: 0.5771\n9288/9288 [==============================] - 0s 24us/sample - loss: 0.5621\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 3s 310us/sample - loss: 4.3220 - val_loss: 2.0522\nEpoch 2/10\n9288/9288 [==============================] - 0s 48us/sample - loss: 1.2781 - val_loss: 0.9416\nEpoch 3/10\n9288/9288 [==============================] - 0s 45us/sample - loss: 0.7194 - val_loss: 0.6952\nEpoch 4/10\n9288/9288 [==============================] - 0s 46us/sample - loss: 0.5975 - val_loss: 0.6333\nEpoch 5/10\n9288/9288 [==============================] - 0s 48us/sample - loss: 0.5684 - val_loss: 0.6132\nEpoch 6/10\n9288/9288 [==============================] - 0s 44us/sample - loss: 0.5570 - val_loss: 0.6032\nEpoch 7/10\n9288/9288 [==============================] - 0s 46us/sample - loss: 0.5510 - val_loss: 0.5950\nEpoch 8/10\n9288/9288 [==============================] - 0s 45us/sample - loss: 0.5457 - val_loss: 0.5896\nEpoch 9/10\n9288/9288 [==============================] - 0s 45us/sample - loss: 0.5444 - val_loss: 0.5851\nEpoch 10/10\n9288/9288 [==============================] - 0s 46us/sample - loss: 0.5408 - val_loss: 0.5819\n2322/2322 [==============================] - 0s 24us/sample - loss: 0.5308\n9288/9288 [==============================] - 0s 25us/sample - loss: 0.5370\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 3s 323us/sample - loss: 3.5098 - val_loss: 1.8576\nEpoch 2/10\n9288/9288 [==============================] - 0s 48us/sample - loss: 1.2212 - val_loss: 0.9916\nEpoch 3/10\n9288/9288 [==============================] - 0s 47us/sample - loss: 0.7782 - val_loss: 0.7832\nEpoch 4/10\n9288/9288 [==============================] - 0s 47us/sample - loss: 0.6712 - val_loss: 0.7186\nEpoch 5/10\n9288/9288 [==============================] - 0s 48us/sample - loss: 0.6348 - val_loss: 0.6908\nEpoch 6/10\n9288/9288 [==============================] - 0s 47us/sample - loss: 0.6171 - val_loss: 0.6727\nEpoch 7/10\n9288/9288 [==============================] - 0s 47us/sample - loss: 0.6048 - val_loss: 0.6584\nEpoch 8/10\n9288/9288 [==============================] - 0s 47us/sample - loss: 0.5950 - val_loss: 0.6462\nEpoch 9/10\n9288/9288 [==============================] - 0s 45us/sample - loss: 0.5846 - val_loss: 0.6352\nEpoch 10/10\n9288/9288 [==============================] - 0s 47us/sample - loss: 0.5786 - val_loss: 0.6259\n2322/2322 [==============================] - 0s 27us/sample - loss: 0.5646\n9288/9288 [==============================] - 0s 25us/sample - loss: 0.5717\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 7s 780us/sample - loss: 3.2949 - val_loss: 2.2381\nEpoch 2/10\n9288/9288 [==============================] - 3s 376us/sample - loss: 1.7647 - val_loss: 1.4365\nEpoch 3/10\n9288/9288 [==============================] - 4s 434us/sample - loss: 1.1822 - val_loss: 1.1091\nEpoch 4/10\n9288/9288 [==============================] - 4s 447us/sample - loss: 0.9100 - val_loss: 0.9472\nEpoch 5/10\n9288/9288 [==============================] - 4s 415us/sample - loss: 0.7844 - val_loss: 0.8694\nEpoch 6/10\n9288/9288 [==============================] - 5s 491us/sample - loss: 0.7243 - val_loss: 0.8248\nEpoch 7/10\n",
                    "name": "stdout"
                },
                {
                    "output_type": "stream",
                    "text": "9288/9288 [==============================] - 3s 364us/sample - loss: 0.6892 - val_loss: 0.7948\nEpoch 8/10\n9288/9288 [==============================] - 3s 355us/sample - loss: 0.6646 - val_loss: 0.7718\nEpoch 9/10\n9288/9288 [==============================] - 5s 491us/sample - loss: 0.6449 - val_loss: 0.7518\nEpoch 10/10\n9288/9288 [==============================] - 4s 390us/sample - loss: 0.6279 - val_loss: 0.7347\n2322/2322 [==============================] - 0s 145us/sample - loss: 0.6133\n9288/9288 [==============================] - 1s 157us/sample - loss: 0.6193\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 8s 886us/sample - loss: 2.5769 - val_loss: 1.5330\nEpoch 2/10\n9288/9288 [==============================] - 3s 360us/sample - loss: 1.0513 - val_loss: 1.0840\nEpoch 3/10\n9288/9288 [==============================] - 6s 617us/sample - loss: 0.8176 - val_loss: 0.9375\nEpoch 4/10\n9288/9288 [==============================] - 3s 355us/sample - loss: 0.7277 - val_loss: 0.8610\nEpoch 5/10\n9288/9288 [==============================] - 3s 351us/sample - loss: 0.6795 - val_loss: 0.8126\nEpoch 6/10\n9288/9288 [==============================] - 4s 461us/sample - loss: 0.6474 - val_loss: 0.7767\nEpoch 7/10\n9288/9288 [==============================] - 4s 422us/sample - loss: 0.6224 - val_loss: 0.7476\nEpoch 8/10\n9288/9288 [==============================] - 4s 387us/sample - loss: 0.6012 - val_loss: 0.7237\nEpoch 9/10\n9288/9288 [==============================] - 5s 490us/sample - loss: 0.5827 - val_loss: 0.7027\nEpoch 10/10\n9288/9288 [==============================] - 3s 347us/sample - loss: 0.5660 - val_loss: 0.6851\n2322/2322 [==============================] - 0s 156us/sample - loss: 0.5992\n9288/9288 [==============================] - 2s 264us/sample - loss: 0.5575\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 7s 735us/sample - loss: 3.1454 - val_loss: 1.8458\nEpoch 2/10\n9288/9288 [==============================] - 3s 355us/sample - loss: 1.3783 - val_loss: 1.1354\nEpoch 3/10\n9288/9288 [==============================] - 5s 493us/sample - loss: 0.9229 - val_loss: 0.9080\nEpoch 4/10\n9288/9288 [==============================] - 3s 354us/sample - loss: 0.7452 - val_loss: 0.8156\nEpoch 5/10\n9288/9288 [==============================] - 3s 353us/sample - loss: 0.6725 - val_loss: 0.7743\nEpoch 6/10\n9288/9288 [==============================] - 4s 476us/sample - loss: 0.6369 - val_loss: 0.7506\nEpoch 7/10\n9288/9288 [==============================] - 3s 368us/sample - loss: 0.6149 - val_loss: 0.7331\nEpoch 8/10\n9288/9288 [==============================] - 3s 372us/sample - loss: 0.5984 - val_loss: 0.7184\nEpoch 9/10\n9288/9288 [==============================] - 5s 549us/sample - loss: 0.5843 - val_loss: 0.7051\nEpoch 10/10\n9288/9288 [==============================] - 4s 385us/sample - loss: 0.5714 - val_loss: 0.6925\n2322/2322 [==============================] - 0s 155us/sample - loss: 0.6168\n9288/9288 [==============================] - 1s 152us/sample - loss: 0.5648\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 7s 789us/sample - loss: 3.1421 - val_loss: 1.8232\nEpoch 2/10\n9288/9288 [==============================] - 4s 388us/sample - loss: 1.3022 - val_loss: 1.1008\nEpoch 3/10\n9288/9288 [==============================] - 3s 347us/sample - loss: 0.8919 - val_loss: 0.9022\nEpoch 4/10\n9288/9288 [==============================] - 5s 531us/sample - loss: 0.7571 - val_loss: 0.8177\nEpoch 5/10\n9288/9288 [==============================] - 3s 371us/sample - loss: 0.6936 - val_loss: 0.7729\nEpoch 6/10\n9288/9288 [==============================] - 4s 379us/sample - loss: 0.6583 - val_loss: 0.7451\nEpoch 7/10\n9288/9288 [==============================] - 5s 512us/sample - loss: 0.6358 - val_loss: 0.7254\nEpoch 8/10\n9288/9288 [==============================] - 4s 399us/sample - loss: 0.6187 - val_loss: 0.7092\nEpoch 9/10\n9288/9288 [==============================] - 4s 481us/sample - loss: 0.6043 - val_loss: 0.6959\nEpoch 10/10\n9288/9288 [==============================] - 3s 354us/sample - loss: 0.5916 - val_loss: 0.6835\n2322/2322 [==============================] - 0s 163us/sample - loss: 0.5807\n9288/9288 [==============================] - 1s 161us/sample - loss: 0.5849\nTrain on 9288 samples, validate on 3870 samples\nEpoch 1/10\n9288/9288 [==============================] - 8s 824us/sample - loss: 2.9819 - val_loss: 1.7396\nEpoch 2/10\n9288/9288 [==============================] - 4s 394us/sample - loss: 1.3308 - val_loss: 1.0095\nEpoch 3/10\n9288/9288 [==============================] - 4s 384us/sample - loss: 0.8241 - val_loss: 0.8036\nEpoch 4/10\n9288/9288 [==============================] - 5s 505us/sample - loss: 0.6872 - val_loss: 0.7345\nEpoch 5/10\n9288/9288 [==============================] - 3s 358us/sample - loss: 0.6372 - val_loss: 0.7017\nEpoch 6/10\n9288/9288 [==============================] - 3s 350us/sample - loss: 0.6099 - val_loss: 0.6789\nEpoch 7/10\n9288/9288 [==============================] - 5s 498us/sample - loss: 0.5891 - val_loss: 0.6604\nEpoch 8/10\n9288/9288 [==============================] - 4s 381us/sample - loss: 0.5721 - val_loss: 0.6446\nEpoch 9/10\n9288/9288 [==============================] - 3s 375us/sample - loss: 0.5568 - val_loss: 0.6308\nEpoch 10/10\n9288/9288 [==============================] - 5s 519us/sample - loss: 0.5427 - val_loss: 0.6179\n2322/2322 [==============================] - 0s 201us/sample - loss: 0.5117\n9288/9288 [==============================] - 2s 165us/sample - loss: 0.5352\nTrain on 11610 samples, validate on 3870 samples\nEpoch 1/10\n11610/11610 [==============================] - 10s 833us/sample - loss: 0.8709 - val_loss: 0.5974\nEpoch 2/10\n11610/11610 [==============================] - 5s 446us/sample - loss: 0.4422 - val_loss: 0.5135\nEpoch 3/10\n11610/11610 [==============================] - 7s 639us/sample - loss: 0.3890 - val_loss: 0.4939\nEpoch 4/10\n11610/11610 [==============================] - 6s 494us/sample - loss: 0.3654 - val_loss: 0.4655\nEpoch 5/10\n11610/11610 [==============================] - 6s 508us/sample - loss: 0.3535 - val_loss: 0.4640\nEpoch 6/10\n11610/11610 [==============================] - 6s 512us/sample - loss: 0.3448 - val_loss: 0.4526\nEpoch 7/10\n11610/11610 [==============================] - 5s 443us/sample - loss: 0.3380 - val_loss: 0.5038\nEpoch 8/10\n11610/11610 [==============================] - 7s 611us/sample - loss: 0.3349 - val_loss: 0.4564\nEpoch 9/10\n11610/11610 [==============================] - 6s 484us/sample - loss: 0.3311 - val_loss: 0.4783\nEpoch 10/10\n11610/11610 [==============================] - 6s 532us/sample - loss: 0.3256 - val_loss: 0.4785\n",
                    "name": "stdout"
                },
                {
                    "output_type": "execute_result",
                    "execution_count": 66,
                    "data": {
                        "text/plain": "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n          estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x7f71b8a342b0>,\n          fit_params=None, iid='warn', n_iter=10, n_jobs=None,\n          param_distributions={'n_hidden': [0, 1, 2, 3], 'n_neurons': array([ 1,  2, ..., 98, 99]), 'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f71b7c74780>},\n          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n          return_train_score='warn', scoring=None, verbose=0)"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "rnd_cv.best_params_",
            "execution_count": 67,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 67,
                    "data": {
                        "text/plain": "{'learning_rate': 0.006485618710550349, 'n_hidden': 3, 'n_neurons': 50}"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "rnd_cv.best_score_",
            "execution_count": 69,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 69,
                    "data": {
                        "text/plain": "-0.34331006009346654"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "final_model = rnd_cv.best_estimator_.model",
            "execution_count": 71,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}