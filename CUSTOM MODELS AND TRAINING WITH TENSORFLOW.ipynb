{
    "cells": [
        {
            "metadata": {
                "collapsed": true
            },
            "cell_type": "markdown",
            "source": "# CUSTOM MODELS AND TRAINING WITH TENSORFLOW"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## USING TENSORFLOW LIKE NUMPY"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import tensorflow as tf",
            "execution_count": 1,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "### TENSORS AND OPERATIONS\ntf.constant([[1., 2., 3.], [4., 5., 6.]])",
            "execution_count": 2,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 2,
                    "data": {
                        "text/plain": "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[1., 2., 3.],\n       [4., 5., 6.]], dtype=float32)>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "tf.constant(42) # scalar",
            "execution_count": 3,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 3,
                    "data": {
                        "text/plain": "<tf.Tensor: shape=(), dtype=int32, numpy=42>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "t = tf.constant([[1., 2., 3.], [4., 5., 6.]])",
            "execution_count": 4,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "t.shape",
            "execution_count": 5,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 5,
                    "data": {
                        "text/plain": "TensorShape([2, 3])"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "t.dtype",
            "execution_count": 6,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 6,
                    "data": {
                        "text/plain": "tf.float32"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "t[:, 1:] # indexing",
            "execution_count": 7,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 7,
                    "data": {
                        "text/plain": "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[2., 3.],\n       [5., 6.]], dtype=float32)>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "t[..., 1, tf.newaxis]",
            "execution_count": 8,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 8,
                    "data": {
                        "text/plain": "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\narray([[2.],\n       [5.]], dtype=float32)>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "t + 10",
            "execution_count": 9,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 9,
                    "data": {
                        "text/plain": "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[11., 12., 13.],\n       [14., 15., 16.]], dtype=float32)>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "tf.square(t)",
            "execution_count": 10,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 10,
                    "data": {
                        "text/plain": "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[ 1.,  4.,  9.],\n       [16., 25., 36.]], dtype=float32)>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "t @ tf.transpose(t)",
            "execution_count": 11,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 11,
                    "data": {
                        "text/plain": "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[14., 32.],\n       [32., 77.]], dtype=float32)>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "tf.multiply(t,10)",
            "execution_count": 12,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 12,
                    "data": {
                        "text/plain": "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[10., 20., 30.],\n       [40., 50., 60.]], dtype=float32)>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "tf.reduce_mean(t)",
            "execution_count": 13,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 13,
                    "data": {
                        "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=3.5>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### TENSORS AND NUMPY"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import numpy as np\na = np.array([2., 4. , 5.])",
            "execution_count": 14,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "tf.constant(a)",
            "execution_count": 15,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 15,
                    "data": {
                        "text/plain": "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "t.numpy()",
            "execution_count": 16,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 16,
                    "data": {
                        "text/plain": "array([[1., 2., 3.],\n       [4., 5., 6.]], dtype=float32)"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "tf.square(a)",
            "execution_count": 17,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 17,
                    "data": {
                        "text/plain": "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([ 4., 16., 25.])>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "np.square(t)",
            "execution_count": 18,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 18,
                    "data": {
                        "text/plain": "array([[ 1.,  4.,  9.],\n       [16., 25., 36.]], dtype=float32)"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### TYPE CONVERSIONS"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "t2 = tf.constant(40., dtype = tf.float64)\ntf.constant(2.0) + tf.cast(t2, tf.float32)",
            "execution_count": 19,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 19,
                    "data": {
                        "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=42.0>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### VARIABLES"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\nv",
            "execution_count": 20,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 20,
                    "data": {
                        "text/plain": "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\narray([[1., 2., 3.],\n       [4., 5., 6.]], dtype=float32)>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "v.assign(2*v)",
            "execution_count": 21,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 21,
                    "data": {
                        "text/plain": "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\narray([[ 2.,  4.,  6.],\n       [ 8., 10., 12.]], dtype=float32)>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "v[0,1].assign(42)",
            "execution_count": 22,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 22,
                    "data": {
                        "text/plain": "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\narray([[ 2., 42.,  6.],\n       [ 8., 10., 12.]], dtype=float32)>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "v[:, 2].assign([0.,1.])",
            "execution_count": 23,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 23,
                    "data": {
                        "text/plain": "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\narray([[ 2., 42.,  0.],\n       [ 8., 10.,  1.]], dtype=float32)>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "v.scatter_nd_update(indices = [[0,0], [1,2]], updates = [100., 200.])",
            "execution_count": 24,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 24,
                    "data": {
                        "text/plain": "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\narray([[100.,  42.,   0.],\n       [  8.,  10., 200.]], dtype=float32)>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## CUSTOM MODELS AND TRAINING ALGORTITHMS"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### CUSTOM LOSS FUNCTIONS"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "def huber_fn(y_true, y_pred):\n    error = y_true - y_pred\n    is_small_error = tf.abs(error) < 1\n    squared_loss = tf.square(error) / 2\n    linear_loss = tf.abs(error) - 0.5\n    return tf.where(is_small_error, squared_loss, linear_loss)",
            "execution_count": 25,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from tensorflow import keras\nimport pandas as pd",
            "execution_count": 26,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sklearn.datasets import load_iris\niris = load_iris()\nx, y = iris[\"data\"], iris[\"target\"]\ny = pd.to_numeric(y, downcast = \"float\")",
            "execution_count": 27,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state = 42, train_size = 0.8)",
            "execution_count": 28,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "input_A = keras.layers.Input(shape = 4)\nhidden_A = keras.layers.Dense(30, activation = \"relu\")(input_A)\nhidden_B = keras.layers.Dense(30, activation = \"relu\")(hidden_A)\nconcat = keras.layers.concatenate([input_A, hidden_B])\noutput = keras.layers.Dense(1, activation = \"softmax\")(concat)\nmodel = keras.Model(inputs = [input_A], outputs = [output])",
            "execution_count": 29,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "model.compile(loss = huber_fn, optimizer = \"nadam\")",
            "execution_count": 30,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "x_train_n, x_val, y_train_n, y_val = train_test_split(x_train, y_train)",
            "execution_count": 31,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "new_model = model.fit(x_train_n, y_train_n, epochs = 10, validation_data = (x_val, y_val))",
            "execution_count": 32,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Train on 90 samples, validate on 30 samples\nEpoch 1/10\n90/90 [==============================] - 5s 50ms/sample - loss: 0.3167 - val_loss: 0.3667\nEpoch 2/10\n90/90 [==============================] - 3s 37ms/sample - loss: 0.3167 - val_loss: 0.3667\nEpoch 3/10\n90/90 [==============================] - 3s 33ms/sample - loss: 0.3167 - val_loss: 0.3667\nEpoch 4/10\n90/90 [==============================] - 4s 41ms/sample - loss: 0.3167 - val_loss: 0.3667\nEpoch 5/10\n90/90 [==============================] - 3s 37ms/sample - loss: 0.3167 - val_loss: 0.3667\nEpoch 6/10\n90/90 [==============================] - 4s 42ms/sample - loss: 0.3167 - val_loss: 0.3667\nEpoch 7/10\n90/90 [==============================] - 3s 35ms/sample - loss: 0.3167 - val_loss: 0.3667\nEpoch 8/10\n90/90 [==============================] - 4s 39ms/sample - loss: 0.3167 - val_loss: 0.3667\nEpoch 9/10\n90/90 [==============================] - 3s 34ms/sample - loss: 0.3167 - val_loss: 0.3667\nEpoch 10/10\n90/90 [==============================] - 3s 33ms/sample - loss: 0.3167 - val_loss: 0.3667\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "def create_huber(threshold=1.0):\n    def huber_fn(y_true, y_pred):\n        error = y_true - y_pred\n        is_small_error = tf.abs(error) < threshold\n        squared_loss = tf.square(error) / 2\n        linear_loss = threshold * tf.abs(error) - threshold**2 / 2\n        return tf.where(is_small_error, squared_loss, linear_loss)\n    return huber_fn",
            "execution_count": 33,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "model.compile(loss = create_huber(2.0), optimizer = \"nadam\")",
            "execution_count": 34,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "class HuberLoss(keras.losses.Loss):\n    def __init__(self, threshold=1.0, **kwargs):\n        self.threshold = threshold\n        super().__init__(**kwargs)\n    def call(self, y_true, y_pred):\n        error = y_true - y_pred\n        is_small_error = tf.abs(error) < self.threshold\n        squared_loss = tf.square(error) / 2\n        linear_loss = self.threshold * tf.abs(error) - self.threshold**2/ 2\n        return tf.where(is_small_error, squared_loss, linear_loss)\n    def get_config(self):\n        base_config = super().get_config()\n        return {**base_config, \"threshold\": self.threshold}",
            "execution_count": 35,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### Custom Activation Functions, Initializers, Regularizers, and Constraints"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "def my_softplus(z): # return value is just tf.nn.softplus(z)\n    return tf.math.log(tf.exp(z) + 1.0)\ndef my_glorot_initializer(shape, dtype=tf.float32):\n    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\ndef my_l1_regularizer(weights):\n    return tf.reduce_sum(tf.abs(0.01 * weights))\ndef my_positive_weights(weights): # return value is just tf.nn.relu(weights)\n    return tf.where(weights < 0., tf.zeros_like(weights), weights)",
            "execution_count": 36,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "layer = keras.layers.Dense(30, activation=my_softplus, kernel_initializer=my_glorot_initializer, kernel_regularizer=my_l1_regularizer, kernel_constraint=my_positive_weights)",
            "execution_count": 37,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "class MyL1Regularizer(keras.regularizers.Regularizer):\n    def __init__(self, factor):\n        self.factor = factor\n    def __call__(self, weights):\n        return tf.reduce_sum(tf.abs(self.factor * weights))\n    def get_config(self):\n        return {\"factor\": self.factor}",
            "execution_count": 38,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### Custom Metrics"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[create_huber(2.0)])",
            "execution_count": 39,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "precision = keras.metrics.Precision()",
            "execution_count": 40,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])",
            "execution_count": 41,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 41,
                    "data": {
                        "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=0.8>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "class HuberMetric(keras.metrics.Metric):\n    def __init__(self, threshold=1.0, **kwargs):\n        super().__init__(**kwargs) # handles base args (e.g., dtype)\n        self.threshold = threshold\n        self.huber_fn = create_huber(threshold)\n        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        metric = self.huber_fn(y_true, y_pred)\n        self.total.assign_add(tf.reduce_sum(metric))\n        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n    def result(self):\n        return self.total / self.count\n    def get_config(self):\n        base_config = super().get_config()\n        return {**base_config, \"threshold\": self.threshold}",
            "execution_count": 42,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### Custom Layers"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "exponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))",
            "execution_count": 43,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "## Redefining the dense layer\nclass MyDense(keras.layers.Layer):\n    def __init__(self, units, activation=None, **kwargs):\n        super().__init__(**kwargs)\n        self.units = units\n        self.activation = keras.activations.get(activation)\n    def build(self, batch_input_shape):\n        self.kernel = self.add_weight(\n        name=\"kernel\", shape=[batch_input_shape[-1], self.units], initializer=\"glorot_normal\")\n        self.bias = self.add_weight(name=\"bias\", shape=[self.units], initializer=\"zeros\")\n        super().build(batch_input_shape) # must be at the end\n    def call(self, X):\n        return self.activation(X @ self.kernel + self.bias)\n    def compute_output_shape(self, batch_input_shape):\n        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n    def get_config(self):\n        base_config = super().get_config()\n        return {**base_config, \"units\": self.units, \"activation\": keras.activations.serialize(self.activation)}",
            "execution_count": 44,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "## Creating a layer with multiple inputs like concatenate\nclass MyMultiLayer(keras.layers.Layer):\n    def call(self, X):\n        X1, X2 = X\n        return [X1 + X2, X1 * X2, X1 / X2]\n    def compute_output_shape(self, batch_input_shape):\n        b1, b2 = batch_input_shape\n        return [b1, b1, b1] # should probably handle broadcasting rules",
            "execution_count": 45,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "## Creating a layer that adds gaussian noise\nclass MyGaussianNoise(keras.layers.Layer):\n    def __init__(self, stddev, **kwargs):\n        super().__init__(**kwargs)\n        self.stddev = stddev\n    def call(self, X, training=None):\n        if training:\n            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n            return X + noise\n        else:\n            return X\n    def compute_output_shape(self, batch_input_shape):\n        return batch_input_shape",
            "execution_count": 46,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### Custom Models"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "## Building a residual block\nclass ResidualBlock(keras.layers.Layer):\n    def __init__(self, n_layers, n_neurons, **kwargs):\n        super().__init__(**kwargs)\n        self.hidden = [keras.layers.Dense(n_neurons, activation=\"elu\", kernel_initializer=\"he_normal\") for _ in range(n_layers)]\n    def call(self, inputs):\n        Z = inputs\n        for layer in self.hidden:\n            Z = layer(Z)\n        return inputs + Z",
            "execution_count": 47,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "class ResidualRegressor(keras.Model):\n    def __init__(self, output_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.hidden1 = keras.layers.Dense(30, activation=\"elu\",\n        kernel_initializer=\"he_normal\")\n        self.block1 = ResidualBlock(2, 30)\n        self.block2 = ResidualBlock(2, 30)\n        self.out = keras.layers.Dense(output_dim)\n    def call(self, inputs):\n        Z = self.hidden1(inputs)\n        for _ in range(1 + 3):\n            Z = self.block1(Z)\n            Z = self.block2(Z)\n        return self.out(Z)",
            "execution_count": 48,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### Losses and Metrics based on Model Internals"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "class ReconstructingRegressor(keras.Model):\n    def __init__(self, output_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.hidden = [keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\") for _ in range(5)]\n        self.out = keras.layers.Dense(output_dim)\n    def build(self, batch_input_shape):\n        n_inputs = batch_input_shape[-1]\n        self.reconstruct = keras.layers.Dense(n_inputs)\n        super().build(batch_input_shape)\n    def call(self, inputs):\n        Z = inputs\n        for layer in self.hidden:\n            Z = layer(Z)\n        reconstruction = self.reconstruct(Z)\n        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n        self.add_loss(0.05 * recon_loss)\n        return self.out(Z)",
            "execution_count": 49,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### Computing Gradients using AutoDiff"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "def f(w1,w2):\n    return 3*w1**2 + 2*w1*w2",
            "execution_count": 50,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "w1, w2 = 5,3\neps = 1e-6\nprint((f(w1+eps,w2)-f(w1,w2))/eps)\nprint((f(w1,w2+eps)-f(w1,w2))/eps)",
            "execution_count": 51,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "36.000003007075065\n10.000000003174137\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "w1, w2 = tf.Variable(5.), tf.Variable(3.)\nwith tf.GradientTape() as tape:\n    z = f(w1,w2)\ngradients = tape.gradient(z, [w1,w2])",
            "execution_count": 52,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "gradients",
            "execution_count": 53,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 53,
                    "data": {
                        "text/plain": "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "with tf.GradientTape(persistent=True) as tape: ## by setting persistent to True, it can be ran multiple times\n    z = f(w1,w2)\ndz_dw1 = tape.gradient(z, w1)\ndz_dw2 = tape.gradient(z, w2)\nprint(dz_dw1, \", \", dz_dw2)",
            "execution_count": 54,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "tf.Tensor(36.0, shape=(), dtype=float32) ,  tf.Tensor(10.0, shape=(), dtype=float32)\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "del tape",
            "execution_count": 55,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "## An attempt to use any other thing other than variables yield None\nc1, c2 = tf.constant(5.), tf.constant(3.)\nwith tf.GradientTape() as tape:\n    z = f(c1, c2)\ntape.gradient(z, [c1, c2]) # returns [None, None] ",
            "execution_count": 56,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 56,
                    "data": {
                        "text/plain": "[None, None]"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "## tape watch forces tape to use any kinds of tensors\nwith tf.GradientTape() as tape:\n    tape.watch(c1)\n    tape.watch(c2)\n    z = f(c1, c2)\ntape.gradient(z, [c1, c2]) # returns [tensor 36., tensor 10.]",
            "execution_count": 57,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 57,
                    "data": {
                        "text/plain": "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "## tf.stop_gradient prevents backpropagation\ndef f(w1, w2):\n    return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2)\nwith tf.GradientTape() as tape:\n    z = f(w1, w2) # same result as without stop_gradient()\ntape.gradient(z, [w1, w2])",
            "execution_count": 58,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 58,
                    "data": {
                        "text/plain": "[<tf.Tensor: shape=(), dtype=float32, numpy=30.0>, None]"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "## to stabilize the softplus approach and ensure the gradients doesn't return NaN\n@tf.custom_gradient\ndef my_better_softplus(z):\n    exp = tf.exp(z)\n    def my_softplus_gradients(grad):\n        return grad / (1 + 1 / exp)\n    return tf.math.log(exp + 1), my_softplus_gradients",
            "execution_count": 59,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### Custom Training Loop"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "l2_reg = keras.regularizers.l2(0.05)\nmodel = keras.models.Sequential([keras.layers.Dense(30, activation = \"elu\", kernel_initializer = \"he_normal\", kernel_regularizer = l2_reg),\n                                 keras.layers.Dense(1, kernel_regularizer = l2_reg)])",
            "execution_count": 60,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "def random_batch(X, y, batch_size = 32):\n    idx = np.random.randint(len(X), size = batch_size)\n    return X[idx], y[idx]",
            "execution_count": 61,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "def print_status_bar(iteration, total, loss, metrics=None):\n    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result()) for m in [loss] + (metrics or [])])\n    end = \"\" if iteration < total else \"\\n\"\n    print(\"\\r{}/{} - \".format(iteration, total) + metrics, end=end)\n",
            "execution_count": 62,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "n_epochs = 5\nbatch_size = 32\nn_steps = len(x_train)//batch_size\noptimizer = keras.optimizers.Nadam(lr=0.01)\nloss_fn = keras.losses.mean_squared_error\nmean_loss = keras.metrics.Mean()\nmetrics = [keras.metrics.MeanAbsoluteError()]",
            "execution_count": 63,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "for epoch in range(1, n_epochs + 1):\n    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n    for step in range(1, n_steps + 1):\n        X_batch, y_batch = random_batch(x_train, y_train)\n        with tf.GradientTape() as tape:\n            y_pred = model(X_batch, training=True)\n            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n            loss = tf.add_n([main_loss] + model.losses)\n        gradients = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n        mean_loss(loss)\n    for metric in metrics:\n        metric(y_batch, y_pred)\n        print_status_bar(step * batch_size, len(y_train), mean_loss,metrics)\n    print_status_bar(len(y_train), len(y_train), mean_loss, metrics)\n    for metric in [mean_loss] + metrics:\n        metric.reset_states()",
            "execution_count": 64,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Epoch 1/5\nWARNING:tensorflow:Layer dense_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n\n120/120 - mean: 48.5687 - mean_absolute_error: 5.9935\nEpoch 2/5\n120/120 - mean: 26.2081 - mean_absolute_error: 4.0712\nEpoch 3/5\n120/120 - mean: 8.4342 - mean_absolute_error: 2.1310\nEpoch 4/5\n120/120 - mean: 5.6094 - mean_absolute_error: 1.8408\nEpoch 5/5\n120/120 - mean: 3.7176 - mean_absolute_error: 1.1148\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### TensorFlow Functions and Graphs"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "def cube(x):\n    return x**3",
            "execution_count": 65,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "cube(2)",
            "execution_count": 66,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 66,
                    "data": {
                        "text/plain": "8"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "cube(tf.constant([2.0]))",
            "execution_count": 67,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 67,
                    "data": {
                        "text/plain": "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([8.], dtype=float32)>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "tf_cube = tf.function(cube)\ntf_cube",
            "execution_count": 68,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 68,
                    "data": {
                        "text/plain": "<tensorflow.python.eager.def_function.Function at 0x7f6b981f1950>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "tf_cube(2)",
            "execution_count": 69,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 69,
                    "data": {
                        "text/plain": "<tf.Tensor: shape=(), dtype=int32, numpy=8>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "tf_cube(tf.constant(2.0))",
            "execution_count": 70,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 70,
                    "data": {
                        "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "@tf.function\ndef tf_cube(x):\n    return x**3",
            "execution_count": 71,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "tf_cube.python_function(2)",
            "execution_count": 72,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 72,
                    "data": {
                        "text/plain": "8"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.7",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.7.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}