{
    "cells": [
        {
            "metadata": {
                "collapsed": true
            },
            "cell_type": "markdown",
            "source": "# Loading and Preprocessing data with Tensorflow"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport pandas as pd\nimport os",
            "execution_count": 1,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## The Data API"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "X = tf.range(10)\ndataset = tf.data.Dataset.from_tensor_slices(X)\ndataset",
            "execution_count": 2,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 2,
                    "data": {
                        "text/plain": "<TensorSliceDataset shapes: (), types: tf.int32>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "for item in dataset:\n    print(item)",
            "execution_count": 3,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "tf.Tensor(0, shape=(), dtype=int32)\ntf.Tensor(1, shape=(), dtype=int32)\ntf.Tensor(2, shape=(), dtype=int32)\ntf.Tensor(3, shape=(), dtype=int32)\ntf.Tensor(4, shape=(), dtype=int32)\ntf.Tensor(5, shape=(), dtype=int32)\ntf.Tensor(6, shape=(), dtype=int32)\ntf.Tensor(7, shape=(), dtype=int32)\ntf.Tensor(8, shape=(), dtype=int32)\ntf.Tensor(9, shape=(), dtype=int32)\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Chaining Transformations"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "dataset = dataset.repeat(3).batch(7)\nfor item in dataset:\n    print(item)",
            "execution_count": 4,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\ntf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\ntf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\ntf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\ntf.Tensor([8 9], shape=(2,), dtype=int32)\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "dataset = dataset.map(lambda x: x*2)",
            "execution_count": 5,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "dataset = dataset.apply(tf.data.experimental.unbatch())",
            "execution_count": 6,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "WARNING:tensorflow:From <ipython-input-6-15e8a30c9761>:1: unbatch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.data.Dataset.unbatch()`.\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "dataset = dataset.filter(lambda x: x< 10)",
            "execution_count": 7,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "for item in dataset.take(3):\n    print(item)",
            "execution_count": 8,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "tf.Tensor(0, shape=(), dtype=int32)\ntf.Tensor(2, shape=(), dtype=int32)\ntf.Tensor(4, shape=(), dtype=int32)\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Shuffling the Data"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "dataset = tf.data.Dataset.range(10).repeat(3)\ndataset = dataset.shuffle(buffer_size = 5, seed = 42).batch(7)\nfor item in dataset:\n    print(item)",
            "execution_count": 9,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\ntf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\ntf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\ntf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\ntf.Tensor([3 6], shape=(2,), dtype=int64)\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nhousing = fetch_california_housing()\nX_train_full, X_test, y_train_full, y_test = train_test_split(\n    housing.data, housing.target.reshape(-1, 1), random_state=42)\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X_train_full, y_train_full, random_state=42)\n\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_mean = scaler.mean_\nX_std = scaler.scale_",
            "execution_count": 10,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n    housing_dir = os.path.join(\"datasets\", \"housing\")\n    os.makedirs(housing_dir, exist_ok=True)\n    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n\n    filepaths = []\n    m = len(data)\n    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n        part_csv = path_format.format(name_prefix, file_idx)\n        filepaths.append(part_csv)\n        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n            if header is not None:\n                f.write(header)\n                f.write(\"\\n\")\n            for row_idx in row_indices:\n                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n                f.write(\"\\n\")\n    return filepaths",
            "execution_count": 11,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "train_data = np.c_[X_train, y_train]\nvalid_data = np.c_[X_valid, y_valid]\ntest_data = np.c_[X_test, y_test]\nheader_cols = housing.feature_names + [\"MedianHouseValue\"]\nheader = \",\".join(header_cols)\n\ntrain_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts=20)\nvalid_filepaths = save_to_multiple_csv_files(valid_data, \"valid\", header, n_parts=10)\ntest_filepaths = save_to_multiple_csv_files(test_data, \"test\", header, n_parts=10)",
            "execution_count": 12,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "train_filepaths",
            "execution_count": 13,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 13,
                    "data": {
                        "text/plain": "['datasets/housing/my_train_00.csv',\n 'datasets/housing/my_train_01.csv',\n 'datasets/housing/my_train_02.csv',\n 'datasets/housing/my_train_03.csv',\n 'datasets/housing/my_train_04.csv',\n 'datasets/housing/my_train_05.csv',\n 'datasets/housing/my_train_06.csv',\n 'datasets/housing/my_train_07.csv',\n 'datasets/housing/my_train_08.csv',\n 'datasets/housing/my_train_09.csv',\n 'datasets/housing/my_train_10.csv',\n 'datasets/housing/my_train_11.csv',\n 'datasets/housing/my_train_12.csv',\n 'datasets/housing/my_train_13.csv',\n 'datasets/housing/my_train_14.csv',\n 'datasets/housing/my_train_15.csv',\n 'datasets/housing/my_train_16.csv',\n 'datasets/housing/my_train_17.csv',\n 'datasets/housing/my_train_18.csv',\n 'datasets/housing/my_train_19.csv']"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed = 42)",
            "execution_count": 14,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "pd.read_csv(train_filepaths[0]).head()",
            "execution_count": 15,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 15,
                    "data": {
                        "text/plain": "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n0  3.5214      15.0  3.049945   1.106548      1447.0  1.605993     37.63   \n1  5.3275       5.0  6.490060   0.991054      3464.0  3.443340     33.69   \n2  3.1000      29.0  7.542373   1.591525      1328.0  2.250847     38.44   \n3  7.1736      12.0  6.289003   0.997442      1054.0  2.695652     33.55   \n4  2.0549      13.0  5.312457   1.085092      3297.0  2.244384     33.93   \n\n   Longitude  MedianHouseValue  \n0    -122.43             1.442  \n1    -117.39             1.687  \n2    -122.98             1.621  \n3    -117.70             2.621  \n4    -116.93             0.956  ",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MedInc</th>\n      <th>HouseAge</th>\n      <th>AveRooms</th>\n      <th>AveBedrms</th>\n      <th>Population</th>\n      <th>AveOccup</th>\n      <th>Latitude</th>\n      <th>Longitude</th>\n      <th>MedianHouseValue</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3.5214</td>\n      <td>15.0</td>\n      <td>3.049945</td>\n      <td>1.106548</td>\n      <td>1447.0</td>\n      <td>1.605993</td>\n      <td>37.63</td>\n      <td>-122.43</td>\n      <td>1.442</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5.3275</td>\n      <td>5.0</td>\n      <td>6.490060</td>\n      <td>0.991054</td>\n      <td>3464.0</td>\n      <td>3.443340</td>\n      <td>33.69</td>\n      <td>-117.39</td>\n      <td>1.687</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.1000</td>\n      <td>29.0</td>\n      <td>7.542373</td>\n      <td>1.591525</td>\n      <td>1328.0</td>\n      <td>2.250847</td>\n      <td>38.44</td>\n      <td>-122.98</td>\n      <td>1.621</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7.1736</td>\n      <td>12.0</td>\n      <td>6.289003</td>\n      <td>0.997442</td>\n      <td>1054.0</td>\n      <td>2.695652</td>\n      <td>33.55</td>\n      <td>-117.70</td>\n      <td>2.621</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0549</td>\n      <td>13.0</td>\n      <td>5.312457</td>\n      <td>1.085092</td>\n      <td>3297.0</td>\n      <td>2.244384</td>\n      <td>33.93</td>\n      <td>-116.93</td>\n      <td>0.956</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "n_readers = 5\ndataset = filepath_dataset.interleave(lambda filepath: tf.data.TextLineDataset(filepath).skip(1), cycle_length=n_readers)",
            "execution_count": 16,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "for line in dataset.take(5):\n    print(line.numpy())",
            "execution_count": 17,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "b'4.2083,44.0,5.323204419889502,0.9171270718232044,846.0,2.3370165745856353,37.47,-122.2,2.782'\nb'4.1812,52.0,5.701388888888889,0.9965277777777778,692.0,2.4027777777777777,33.73,-118.31,3.215'\nb'3.6875,44.0,4.524475524475524,0.993006993006993,457.0,3.195804195804196,34.04,-118.15,1.625'\nb'3.3456,37.0,4.514084507042254,0.9084507042253521,458.0,3.2253521126760565,36.67,-121.7,2.526'\nb'3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442'\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Preprocessing the data"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "##X_mean, X_std = [...] # mean and scale of each feature in the training  set\nn_inputs = 8\ndef preprocess(line):\n    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n    fields = tf.io.decode_csv(line, record_defaults=defs)\n    x = tf.stack(fields[:-1])\n    y = tf.stack(fields[-1:])\n    return (x - X_mean) / X_std, y",
            "execution_count": 18,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')",
            "execution_count": 19,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 19,
                    "data": {
                        "text/plain": "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n array([ 0.16579159,  1.216324  , -0.05204564, -0.39215982, -0.5277444 ,\n        -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)>,\n <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.782], dtype=float32)>)"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "def csv_reader_dataset(filepaths, repeat=1, n_readers=5,n_read_threads=None, shuffle_buffer_size=10000, n_parse_threads=5, batch_size=32):\n    dataset = tf.data.Dataset.list_files(filepaths)\n    dataset = dataset.interleave(lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n    cycle_length=n_readers, num_parallel_calls=n_read_threads)\n    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n    dataset = dataset.shuffle(shuffle_buffer_size).repeat(repeat)\n    return dataset.batch(batch_size).prefetch(1)",
            "execution_count": 20,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Using the dataset with tf.keras"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "train_set = csv_reader_dataset(train_filepaths)\ntest_set = csv_reader_dataset(test_filepaths)\nvalid_set = csv_reader_dataset(valid_filepaths)",
            "execution_count": 21,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "model = keras.models.Sequential([\n    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n    keras.layers.Dense(1),\n])",
            "execution_count": 22,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "model.compile(loss = \"mse\", optimizer = keras.optimizers.SGD(lr = 1e-3))",
            "execution_count": 23,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "batch_size = 32\nmodel.fit(train_set, steps_per_epoch=(len(X_train) // batch_size)-200, epochs=3, validation_data=valid_set)",
            "execution_count": 24,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Train for 162 steps\nEpoch 1/3\n162/162 [==============================] - 85s 523ms/step - loss: 2.8490 - val_loss: 7.0206\nEpoch 2/3\n162/162 [==============================] - 85s 527ms/step - loss: 1.3721 - val_loss: 1.4105\nEpoch 3/3\n 39/162 [======>.......................] - ETA: 46s - loss: 0.9515WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 486 batches). You may need to use the repeat() function when building your dataset.\n 39/162 [======>.......................] - ETA: 46s - loss: 0.9514",
                    "name": "stdout"
                },
                {
                    "output_type": "execute_result",
                    "execution_count": 24,
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f47cc65d290>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "model.evaluate(test_set, steps=len(X_test) // batch_size)",
            "execution_count": 28,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "161/161 [==============================] - 34s 211ms/step - loss: 0.9371\n",
                    "name": "stdout"
                },
                {
                    "output_type": "execute_result",
                    "execution_count": 28,
                    "data": {
                        "text/plain": "0.9370764447665363"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "new_set = test_set.take(3).map(lambda X, y : X)",
            "execution_count": 29,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "model.predict(new_set);",
            "execution_count": 30,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "optimizer = keras.optimizers.Nadam(lr=0.01)\nloss_fn = keras.losses.mean_squared_error\n\nn_epochs = 5\nbatch_size = 32\nn_steps_per_epoch = len(X_train) // batch_size\ntotal_steps = n_epochs * n_steps_per_epoch\nglobal_step = 0\nfor X_batch, y_batch in train_set.take(total_steps):\n    global_step += 1\n    print(\"\\rGlobal step {}/{}\".format(global_step, total_steps), end=\"\")\n    with tf.GradientTape() as tape:\n        y_pred = model(X_batch)\n        main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n        loss = tf.add_n([main_loss] + model.losses)\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))",
            "execution_count": 31,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Global step 363/1810",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "@tf.function\ndef train(model, optimizer, loss_fn, n_epochs):\n    train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs)\n    for X_batch, y_batch in train_set:\n        with tf.GradientTape() as tape:\n            y_pred = model(X_batch)\n            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n            loss = tf.add_n([main_loss] + model.losses)\n        grads = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))",
            "execution_count": 33,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.7",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.7.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}